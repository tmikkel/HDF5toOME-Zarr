{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b7f60d-33fe-4946-8a89-e7861ed912a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requires Zarr version less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf989794-5d76-4844-847c-a25030704e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported succesfully!\n",
      "2.18.7\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import zarr\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask.diagnostics import ProgressBar\n",
    "import numpy as np\n",
    "from numcodecs import Blosc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"All libraries imported succesfully!\")\n",
    "print(zarr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0ce794f-8f79-4e79-8e0c-493caf3f36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion arguments\n",
    "#input_path = \"/dtu/3d-imaging-center/projects/2023_CoM-BraiN/analysis/OME_Output/small_wMB_4bin.h5\"\n",
    "#output_path = \"/dtu/3d-imaging-center/projects/2023_CoM-BraiN/analysis/OME_Output/contiguous_test1.ome.zarr\"\n",
    "input_path = \"/Users/tobiasschleiss/documents/dtu/thesis/input/new_small_wMB_4bin.h5\"\n",
    "output_path = \"/Users/tobiasschleiss/Documents/DTU/Thesis/output/contiguous_test2.ome.zarr\"\n",
    "\n",
    "target_chunks = (64, 64, 64)\n",
    "dataset_path = 'exchange/data'\n",
    "compression_level=3\n",
    "\n",
    "no_seeks = False\n",
    "\n",
    "if no_seeks == True:\n",
    "    block_shape = (64, 1280, 1280) #Approximatly 420MB with no disk seeks\n",
    "else:\n",
    "    block_shape = (1280, 1280, 64) #Approximatly 420MB with many disk seeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11832fc6-e369-4de0-9135-373751944554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (1280, 1280, 1280)\n",
      "  Dtype: float32\n",
      "  Size: 7.81 GB\n"
     ]
    }
   ],
   "source": [
    "# Inspect HDF5 file\n",
    "with h5py.File(input_path, 'r') as f:\n",
    "    if dataset_path not in f:\n",
    "        print(f\"  ERROR: Dataset '{dataset_path}' not found\")\n",
    "        print(f\"  Available paths: {list(f.keys())}\")\n",
    "        \n",
    "    dataset = f[dataset_path]\n",
    "    shape = dataset.shape\n",
    "    dtype = dataset.dtype\n",
    "    data_size_gb = dataset.nbytes / (1024**3)\n",
    "    dtype_size = dtype.itemsize\n",
    "        \n",
    "    print(f\"  Shape: {shape}\")\n",
    "    print(f\"  Dtype: {dtype}\")\n",
    "    print(f\"  Size: {data_size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9128d45f-05f1-42fc-8d87-41ae950009cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: HDF5 -> Level 0 zarr (optimized for contigous reads)\n",
      "Selected block shape: (1280, 1280, 64)\n",
      "Processing 20 blocks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_30191/2102984051.py:14: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n",
      "  store = zarr.NestedDirectoryStore(output_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Block    1/20 (  5.0%) -   1.1 blocks/s - ETA:     17s\n",
      "  Block    2/20 ( 10.0%) -   1.1 blocks/s - ETA:     16s\n",
      "  Block    3/20 ( 15.0%) -   1.1 blocks/s - ETA:     15s\n",
      "  Block    4/20 ( 20.0%) -   1.1 blocks/s - ETA:     14s\n",
      "  Block    5/20 ( 25.0%) -   1.1 blocks/s - ETA:     13s\n",
      "  Block    6/20 ( 30.0%) -   1.1 blocks/s - ETA:     12s\n",
      "  Block    7/20 ( 35.0%) -   1.1 blocks/s - ETA:     12s\n",
      "  Block    8/20 ( 40.0%) -   1.1 blocks/s - ETA:     11s\n",
      "  Block    9/20 ( 45.0%) -   1.1 blocks/s - ETA:     10s\n",
      "  Block   10/20 ( 50.0%) -   1.1 blocks/s - ETA:      9s\n",
      "  Block   11/20 ( 55.0%) -   1.1 blocks/s - ETA:      8s\n",
      "  Block   12/20 ( 60.0%) -   1.1 blocks/s - ETA:      7s\n",
      "  Block   13/20 ( 65.0%) -   1.1 blocks/s - ETA:      6s\n",
      "  Block   14/20 ( 70.0%) -   1.1 blocks/s - ETA:      6s\n",
      "  Block   15/20 ( 75.0%) -   1.1 blocks/s - ETA:      5s\n",
      "  Block   16/20 ( 80.0%) -   1.1 blocks/s - ETA:      4s\n",
      "  Block   17/20 ( 85.0%) -   1.1 blocks/s - ETA:      3s\n",
      "  Block   18/20 ( 90.0%) -   1.1 blocks/s - ETA:      2s\n",
      "  Block   19/20 ( 95.0%) -   1.1 blocks/s - ETA:      1s\n",
      "  Block   20/20 (100.0%) -   1.1 blocks/s - ETA:      0s\n",
      "\n",
      "✓ Level 0 complete in 18.7s\n",
      "  0:00:18\n",
      "  Throughput: 0.45 GB/s\n"
     ]
    }
   ],
   "source": [
    "print(\"Stage 1: HDF5 -> Level 0 zarr (optimized for contigous reads)\")\n",
    "\n",
    "# Open HDF5\n",
    "with h5py.File(input_path, 'r') as f:\n",
    "    dataset = f[dataset_path]\n",
    "    shape = dataset.shape\n",
    "    dtype = dataset.dtype \n",
    "\n",
    "    block_z, block_y, block_x = block_shape\n",
    "    print(f\"Selected block shape: {block_shape}\")\n",
    "    z_total, y_total, x_total = shape\n",
    "        \n",
    "    # Setup output zarr store    \n",
    "    store = zarr.NestedDirectoryStore(output_path)\n",
    "    root = zarr.open_group(store=store, mode='w')\n",
    "\n",
    "    # Compressor for all levels\n",
    "    compressor = Blosc(cname='zstd', clevel=compression_level, shuffle=Blosc.BITSHUFFLE)\n",
    "        \n",
    "    level_0 = root.create_dataset(\n",
    "        '0',\n",
    "        shape=shape,\n",
    "        chunks=target_chunks,\n",
    "        dtype=dtype,\n",
    "        compressor=compressor\n",
    "    )\n",
    "\n",
    "    level0_start = time.time()\n",
    "    block_count = 0\n",
    "        \n",
    "    # Calculate total blocks\n",
    "    total_blocks = (\n",
    "        ((z_total + block_z - 1) // block_z) *\n",
    "        ((y_total + block_y - 1) // block_y) *\n",
    "        ((x_total + block_x - 1) // block_x)\n",
    "    )\n",
    "        \n",
    "    print(f\"Processing {total_blocks} blocks...\")\n",
    "\n",
    "    # Iterate over dataset\n",
    "    for z_start in range(0, z_total, block_z):\n",
    "        z_end = min(z_start + block_z, z_total)\n",
    "            \n",
    "        for y_start in range(0, y_total, block_y):\n",
    "            y_end = min(y_start + block_y, y_total)\n",
    "\n",
    "            for x_start in range(0, x_total, block_x):\n",
    "                x_end = min(x_start + block_x, x_total)\n",
    "                    \n",
    "                block_count += 1\n",
    "                        \n",
    "                # Read block from HDF5\n",
    "                block = dataset[z_start:z_end, y_start:y_end, x_start:x_end]\n",
    "                        \n",
    "                # Write to zarr (zarr will internally chunk to target_chunks)\n",
    "                level_0[z_start:z_end, y_start:y_end, x_start:x_end] = block\n",
    "                        \n",
    "                del block\n",
    "                        \n",
    "                # Progress reporting\n",
    "                if block_count % 1 == 0 or block_count == total_blocks:\n",
    "                    elapsed = time.time() - level0_start\n",
    "                    rate = block_count / elapsed if elapsed > 0 else 0\n",
    "                    eta = (total_blocks - block_count) / rate if rate > 0 else 0\n",
    "                    progress = block_count / total_blocks * 100\n",
    "                        \n",
    "                    print(f\"  Block {block_count:4d}/{total_blocks} ({progress:5.1f}%) - \"\n",
    "                            f\"{rate:5.1f} blocks/s - ETA: {eta:6.0f}s\")   \n",
    "        \n",
    "    elapsed_level0 = time.time() - level0_start\n",
    "    throughput = (np.prod(shape) * dtype_size / 1e9) / elapsed\n",
    "        \n",
    "    print(f\"\\n✓ Level 0 complete in {elapsed_level0:.1f}s\")\n",
    "    print(f\"  {timedelta(seconds=int(elapsed_level0))}\")\n",
    "    print(f\"  Throughput: {throughput:.2f} GB/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5731b4-ab57-4337-80e7-75be77913c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
