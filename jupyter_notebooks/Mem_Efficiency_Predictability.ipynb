{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d9bdc-8b96-4dd0-b11b-b798d8f8e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requires Zarr version less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0320a62-6d8e-4315-85c9-5091b1408fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported succesfully!\n",
      "2.18.7\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import zarr\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import dask\n",
    "from dask import delayed\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "from numcodecs import Blosc\n",
    "from pathlib import Path\n",
    "import dask_memusage\n",
    "import os\n",
    "\n",
    "from dask.distributed import Client, LocalCluster, as_completed, wait\n",
    "\n",
    "print(\"All libraries imported succesfully!\")\n",
    "print(zarr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe157bf2-912e-4128-aa22-6d5192b072ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem per worker (bytes): 11200000000.0\n",
      "Mem per worker (GB): 11.2\n"
     ]
    }
   ],
   "source": [
    "# Conversion arguments\n",
    "\n",
    "#input_path = \"/dtu/3d-imaging-center/projects/2023_CoM-BraiN/analysis/pipelineTestData/wMB_4bin.h5\"\n",
    "#output_path = \"/dtu/3d-imaging-center/projects/2023_CoM-BraiN/analysis/OME_Output/predict_test3.ome.zarr\"\n",
    "#log_path = Path(\"/dtu/3d-imaging-center/projects/2023_CoM-BraiN/analysis/OME_Output/\") / \"memusage.csv\"  # Save in output folder\n",
    "\n",
    "input_path = \"/Users/tobiasschleiss/documents/dtu/thesis/input/small_wMB_4bin.h5\"\n",
    "output_path = \"/Users/tobiasschleiss/Documents/DTU/Thesis/output/direct.ome.zarr\"\n",
    "log_path = Path(\"/Users/tobiasschleiss/Documents/DTU/Thesis/output\") / \"memusage.csv\"  # Save in output folder\n",
    "\n",
    "target_chunks = (64, 64, 64)\n",
    "dataset_path = 'exchange/data'\n",
    "max_mem_gb=14\n",
    "downsample_factor=8\n",
    "compression_level=3\n",
    "target_top_level_mb=100\n",
    "safety_factor = 0.80\n",
    "\n",
    "available_mem_bytes = max_mem_gb * 1e9 * safety_factor\n",
    "\n",
    "n_workers = 1\n",
    "threads_per_worker=1\n",
    "memory_limit = available_mem_bytes/n_workers\n",
    "print(f\"Mem per worker (bytes): {memory_limit}\")\n",
    "print(f\"Mem per worker (GB): {(memory_limit/1e9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9413768-4fb1-4eeb-a8e1-5aa1e88f84b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (1651, 2200, 2200)\n",
      "  Dtype: float32\n",
      "  Size: 29.77 GB\n",
      "  HDF5 chunks: Contiguous\n"
     ]
    }
   ],
   "source": [
    "# Inspect HDF5 file\n",
    "with h5py.File(input_path, 'r') as f:\n",
    "    if dataset_path not in f:\n",
    "        print(f\"  ERROR: Dataset '{dataset_path}' not found\")\n",
    "        print(f\"  Available paths: {list(f.keys())}\")\n",
    "        \n",
    "    dataset = f[dataset_path]\n",
    "    shape = dataset.shape\n",
    "    dtype = dataset.dtype\n",
    "    h5_chunks = dataset.chunks\n",
    "    data_size_gb = dataset.nbytes / (1024**3)\n",
    "    data_size_mb = dataset.nbytes / (1024**2)\n",
    "    dtype_size = dtype.itemsize\n",
    "        \n",
    "    print(f\"  Shape: {shape}\")\n",
    "    print(f\"  Dtype: {dtype}\")\n",
    "    print(f\"  Size: {data_size_gb:.2f} GB\")\n",
    "    print(f\"  HDF5 chunks: {h5_chunks if h5_chunks else 'Contiguous'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbfaf7ed-38c9-42e3-b856-09af9ee7902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target top level: 100 MB\n",
      "Recommended levels: 2\n",
      "Actual top level: 59.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate levels needed\n",
    "levels = 1\n",
    "current_size_mb = data_size_mb\n",
    "    \n",
    "while current_size_mb > target_top_level_mb:\n",
    "    current_size_mb = current_size_mb / (downsample_factor ** 3)\n",
    "    levels += 1\n",
    "\n",
    "print(f\"Target top level: {target_top_level_mb} MB\")\n",
    "print(f\"Recommended levels: {levels}\")\n",
    "print(f\"Actual top level: {current_size_mb:.1f} MB\")\n",
    "\n",
    "pyramid_levels = levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbaadd67-ae1f-4780-a69e-8e74b92fe5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Optimal Block Size Calculation\n",
      "============================================================\n",
      "Memory budget: 11.20 GB (using 80%)\n",
      "Available for block: 7.07 GB\n",
      "Actual block size: 1.24 GB\n",
      "============================================================\n",
      "Read chunks: (64, 2200, 2200)\n"
     ]
    }
   ],
   "source": [
    "#Dynamic block calculation based on memory per worker.\n",
    "     \n",
    "z, y, x = shape\n",
    "chunk_z, chunk_y, chunk_x = target_chunks\n",
    "\n",
    "\n",
    "available_bytes = memory_limit * (2/3) - 400_000_000\n",
    "\n",
    "    \n",
    "# Calculate maximum amount of Z-planes that fit in memory\n",
    "bytes_per_z_plane = y * x * dtype_size\n",
    "max_z_planes = int(available_bytes / bytes_per_z_plane)\n",
    "\n",
    "\n",
    "if max_z_planes < chunk_z:\n",
    "    print(f\"\\nFull Target Z plane ({target_chunks[0]}) too large for memory\")\n",
    "    print(\"Reducing Y axis to fit block in memory\")\n",
    "    \n",
    "    # Calculate max Y that fits with target Z and full X\n",
    "    bytes_per_y_row = chunk_z * x * dtype_size\n",
    "    max_y_rows = int(available_bytes / bytes_per_y_row)\n",
    "\n",
    "    if max_y_rows < chunk_y:\n",
    "        print(f\"\\nFull Target Y rows ({target_chunks[1]}) too large for memory\")\n",
    "        print(\"Reducing X axis to fit block in memory\")\n",
    "        bytes_per_x_column = chunk_z * chunk_y * dtype_size\n",
    "        max_x_columns = int(available_bytes / bytes_per_x_column)\n",
    "        optimal_x = (max_x_columns // chunk_x) * chunk_x\n",
    "        optimal_x = max(chunk_x, optimal_x)  # At least one chunk depth\n",
    "        if max_x_columns >= x / 2 + chunk_x:\n",
    "            optimal_x = int(min(optimal_x, ((x / 2) // chunk_x) * chunk_x + chunk_x))   # Cap to a multiple of chunk_y just above half of y\n",
    "        x = optimal_x\n",
    "\n",
    "    optimal_y = (max_y_rows // chunk_y) * chunk_y \n",
    "    optimal_y = max(chunk_y, optimal_y)  # At least one chunk depth\n",
    "    if max_y_rows >= y / 2 + chunk_y:\n",
    "        optimal_y = int(min(optimal_y, ((y / 2) // chunk_y) * chunk_y + chunk_y))   # Cap to a multiple of chunk_y just above half of y\n",
    "    y = optimal_y\n",
    "\n",
    "block_shape = chunk_z, y, x\n",
    "    \n",
    "# Calculate actual memory usage\n",
    "actual_gb = chunk_z * y * x * dtype_size / 1e9\n",
    "\n",
    "max_gb = available_mem_bytes / 1e9\n",
    "    \n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Optimal Block Size Calculation\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Memory budget: {max_gb:.2f} GB (using {int(safety_factor*100)}%)\")\n",
    "print(f\"Available for block: {available_bytes/1e9:.2f} GB\")\n",
    "print(f\"Actual block size: {actual_gb:.2f} GB\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Read chunks: {block_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05d97fc4-1c60-4824-b50d-1971d89a5ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block shape: (64, 2200, 2200)\n",
      "Number of Workers: 1 memory per worker 11200000000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/node.py:188: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 61097 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask dashboard: http://127.0.0.1:61097/status\n",
      "Memory logging to: /Users/tobiasschleiss/Documents/DTU/Thesis/output/memusage.csv\n",
      "\n",
      "✓ Submitting 26 tasks for parallel execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:32: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n",
      "  store = zarr.NestedDirectoryStore(output_path)\n",
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/180470141.py:51: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed blocks: 26\n",
      "\n",
      "✓ Complete: 116.6s | 0.27 GB/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread WorkerMemory:\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/utils.py\", line 1930, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 560, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 143, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x13c566750>: ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/dask_memusage.py\", line 66, in _fetch_memory\n",
      "    worker_to_mem = client.run(_process_memory)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/client.py\", line 3199, in run\n",
      "    return self.sync(\n",
      "           ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/utils.py\", line 383, in sync\n",
      "    return sync(\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/utils.py\", line 459, in sync\n",
      "    raise error\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/utils.py\", line 433, in f\n",
      "    result = yield future\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/tornado/gen.py\", line 783, in run\n",
      "    value = future.result()\n",
      "            ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/client.py\", line 3076, in _run\n",
      "    responses = await self.scheduler.broadcast(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/core.py\", line 1256, in send_recv_from_rpc\n",
      "    comm = await self.pool.connect(self.addr)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/core.py\", line 1485, in connect\n",
      "    return await self._connect(addr=addr, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:61098 after 30 s\n",
      "2026-02-15 05:53:12,100 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "#HDF5 to level 0\n",
    "\n",
    "with h5py.File(input_path, \"r\") as f:\n",
    "        dataset = f[dataset_path]\n",
    "        shape = dataset.shape\n",
    "        dtype = dataset.dtype\n",
    "        dtype_size = dtype.itemsize\n",
    "        data_size_mb = dataset.nbytes / (1024**2)\n",
    "    \n",
    "        print(f\"block shape: {block_shape}\")\n",
    "\n",
    "        block_z, block_y, block_x = block_shape\n",
    "        z_total, y_total, x_total = shape\n",
    "\n",
    "\n",
    "read_chunks_bytes = np.prod(block_shape) * dtype_size\n",
    "\n",
    "print(f\"Number of Workers: {n_workers} memory per worker {memory_limit}\")\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    n_workers=n_workers,\n",
    "    threads_per_worker=threads_per_worker,\n",
    "    processes=True,\n",
    "    memory_limit=memory_limit\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "\n",
    "dask_memusage.install(cluster.scheduler, str(log_path))\n",
    "print(f\"Memory logging to: {log_path}\")\n",
    "\n",
    "store = zarr.NestedDirectoryStore(output_path)\n",
    "root = zarr.open_group(store, mode=\"w\")\n",
    "compressor = Blosc(cname=\"zstd\", clevel=compression_level, shuffle=Blosc.BITSHUFFLE)\n",
    "\n",
    "root.create_dataset(\n",
    "    \"0\",\n",
    "    shape=shape,\n",
    "    chunks=target_chunks,\n",
    "    dtype=dtype,\n",
    "    compressor=compressor\n",
    ")\n",
    "\n",
    "del root, store\n",
    "\n",
    "@dask.delayed\n",
    "def copy_block(z_start, z_end, y_start, y_end, x_start, x_end):\n",
    "    with h5py.File(input_path, \"r\") as f:\n",
    "        block = f[dataset_path][z_start:z_end, y_start:y_end, x_start:x_end]\n",
    "    \n",
    "    store = zarr.NestedDirectoryStore(output_path)\n",
    "    root = zarr.open_group(store, mode=\"a\")\n",
    "    root[\"0\"][z_start:z_end, y_start:y_end, x_start:x_end] = block\n",
    "\n",
    "    return (z_end - z_start, y_end - y_start, x_end - x_start)\n",
    "\n",
    "tasks = []\n",
    "for z_start in range(0, z_total, block_z):\n",
    "    z_end = min(z_start + block_z, shape[0])\n",
    "\n",
    "    for y_start in range(0, y_total, block_y):\n",
    "        y_end = min(y_start + block_y, y_total)\n",
    "\n",
    "        for x_start in range(0, x_total, block_x):\n",
    "            x_end = min(x_start + block_x, x_total)\n",
    "\n",
    "            tasks.append(copy_block(z_start, z_end, y_start, y_end, x_start, x_end))\n",
    "\n",
    "total_tasks = len(tasks)\n",
    "print(f\"\\n✓ Submitting {total_tasks} tasks for parallel execution...\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Submit all tasks and get futures\n",
    "futures = client.compute(tasks)\n",
    "\n",
    "# Track progress\n",
    "completed = 0\n",
    "\n",
    "for future in as_completed(futures):\n",
    "    completed += 1\n",
    "    elapsed = time.time() - start\n",
    "    rate = completed / elapsed if elapsed > 0 else 0\n",
    "    eta = (total_tasks - completed) / rate if rate > 0 else 0\n",
    "    \n",
    "    print(f\"completed blocks: {completed}\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "total_gb = np.prod(shape) * dtype_size / 1e9\n",
    "\n",
    "print(f\"\\n✓ Complete: {elapsed:.1f}s | {total_gb/elapsed:.2f} GB/s\")\n",
    "\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d342943a-1ad0-4022-923f-2c3ff27e3824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Source chunks: (64, 64, 64)\n",
      "  Source shape: (1651, 2200, 2200)\n",
      "  Source dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Inspection level_0\n",
    "source = da.from_zarr(output_path, component='0')\n",
    "    \n",
    "print(f\"  Source chunks: {source.chunksize}\")\n",
    "print(f\"  Source shape: {source.shape}\")\n",
    "print(f\"  Source dtype: {source.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa898864-1802-4131-8aa3-b38d1ce179b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "217ed5ef-2a2f-499e-95c9-4a250e5bb23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/node.py:188: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 61956 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask dashboard: http://127.0.0.1:61956/status\n",
      "Memory logging to: /Users/tobiasschleiss/Documents/DTU/Thesis/output/memusage.csv\n",
      "============================================================\n",
      "Building OME-Zarr Multi-Resolution Pyramid (Block-Mean)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "LEVEL 1: Block-Mean Downsampling\n",
      "============================================================\n",
      "Previous shape: (1651, 2200, 2200)\n",
      "New shape: (206, 275, 275)\n",
      "Total tasks for current level: 100\n",
      "Total recurring in flights: 0\n",
      "Finished level 1 in 74.6s\n",
      "\n",
      "Total pyramid time: 1.24 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread WorkerMemory:\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/utils.py\", line 1930, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 560, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 143, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15fa33170>: ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/dask_memusage.py\", line 66, in _fetch_memory\n",
      "    worker_to_mem = client.run(_process_memory)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/client.py\", line 3199, in run\n",
      "    return self.sync(\n",
      "           ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/utils.py\", line 383, in sync\n",
      "    return sync(\n",
      "           ^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/utils.py\", line 459, in sync\n",
      "    raise error\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/utils.py\", line 433, in f\n",
      "    result = yield future\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/tornado/gen.py\", line 783, in run\n",
      "    value = future.result()\n",
      "            ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/client.py\", line 3076, in _run\n",
      "    responses = await self.scheduler.broadcast(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/core.py\", line 1256, in send_recv_from_rpc\n",
      "    comm = await self.pool.connect(self.addr)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/core.py\", line 1485, in connect\n",
      "    return await self._connect(addr=addr, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:61957 after 30 s\n",
      "2026-02-15 05:56:38,687 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "#Pyramid Write\n",
    "\n",
    "def mean_downsample_block(\n",
    "    source_path,\n",
    "    destination_path,\n",
    "    block_region,\n",
    "    destination_coords,\n",
    "    downsample_factor\n",
    "):\n",
    "\n",
    "    src = zarr.open(source_path, mode=\"r\")\n",
    "    store = zarr.open(destination_path, mode=\"r+\")\n",
    "\n",
    "    block = src[block_region]\n",
    "\n",
    "    d = downsample_factor\n",
    "\n",
    "    # Trim dimensions to be divisible by downsample factor\n",
    "    block_z = (block.shape[0] // d) * d\n",
    "    block_y = (block.shape[1] // d) * d\n",
    "    block_x = (block.shape[2] // d) * d\n",
    "    block = block[:block_z, :block_y, :block_x]\n",
    "\n",
    "    # Reshape to compute block mean\n",
    "    # Each axis is split into (num_blocks, block_size)\n",
    "    reshaped = block.reshape(\n",
    "        block_z // d, d,\n",
    "        block_y // d, d,\n",
    "        block_x // d, d\n",
    "    )\n",
    "\n",
    "    # Compute mean over the block axes (1,3,5) (downsample the block)\n",
    "    downsampled = reshaped.mean(axis=(1, 3, 5)).astype(block.dtype)\n",
    "\n",
    "    # Write to pyramid array\n",
    "    store[destination_coords] = downsampled\n",
    "\n",
    "\n",
    "def build_level(\n",
    "    client,\n",
    "    output_path,\n",
    "    level,\n",
    "    downsample_factor,\n",
    "    target_chunks,\n",
    "    compressor,\n",
    "    max_in_flight=128\n",
    "):\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"LEVEL {level}: Block-Mean Downsampling\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    source_path = os.path.join(output_path, str(level - 1))\n",
    "    destination_path = os.path.join(output_path, str(level))\n",
    "\n",
    "    #load previous level as source\n",
    "    source = zarr.open(source_path, mode=\"r\")\n",
    "    current_shape = source.shape\n",
    "    new_shape = tuple(max(1, s // downsample_factor) for s in current_shape)\n",
    "\n",
    "    print(f\"Previous shape: {current_shape}\")\n",
    "    print(f\"New shape: {new_shape}\")\n",
    "\n",
    "    # Create destination array\n",
    "    zarr.open(\n",
    "        destination_path,\n",
    "        mode=\"w\",\n",
    "        shape=new_shape,\n",
    "        chunks=target_chunks,\n",
    "        dtype=source.dtype,\n",
    "        compressor=compressor,\n",
    "        dimension_separator=\"/\"\n",
    "    )\n",
    "\n",
    "    futures = []\n",
    "    \n",
    "    chunk_z, chunk_y, chunk_x = target_chunks\n",
    "\n",
    "    current_total_tasks = (\n",
    "        (int(np.ceil(new_shape[0] / chunk_z)))*\n",
    "        (int(np.ceil(new_shape[1] / chunk_y)))*\n",
    "        (int(np.ceil(new_shape[2] / chunk_x)))\n",
    "    )\n",
    "    print(f\"Total tasks for current level: {current_total_tasks}\")\n",
    "\n",
    "    print(f\"Total recurring in flights: {current_total_tasks // max_in_flight}\")\n",
    "\n",
    "    level_start = time.time()\n",
    "    \n",
    "    completed = 0\n",
    "\n",
    "    # Iterate over output blocks\n",
    "    for z_start in range(0, new_shape[0], chunk_z):\n",
    "        for y_start in range(0, new_shape[1], chunk_y):\n",
    "            for x_start in range(0, new_shape[2], chunk_x):\n",
    "\n",
    "                #Tuple holding python slice objects (block write coordinates)\n",
    "                destination_coords = (\n",
    "                    slice(z_start, min(z_start + chunk_z, new_shape[0])),\n",
    "                    slice(y_start, min(y_start + chunk_y, new_shape[1])),\n",
    "                    slice(x_start, min(x_start + chunk_x, new_shape[2])),\n",
    "                )\n",
    "\n",
    "                # Mapping block\n",
    "                source_start = (\n",
    "                    z_start * downsample_factor,\n",
    "                    y_start * downsample_factor,\n",
    "                    x_start * downsample_factor,\n",
    "                )\n",
    "                source_end = (\n",
    "                    min((z_start + chunk_z) * downsample_factor, current_shape[0]),\n",
    "                    min((y_start + chunk_y) * downsample_factor, current_shape[1]),\n",
    "                    min((x_start + chunk_x) * downsample_factor, current_shape[2]),\n",
    "                )\n",
    "\n",
    "                #Tuple holding python slice objects (Block to be read from current array)\n",
    "                block_region = (\n",
    "                    slice(source_start[0], source_end[0]),\n",
    "                    slice(source_start[1], source_end[1]),\n",
    "                    slice(source_start[2], source_end[2])\n",
    "                )\n",
    "\n",
    "                # Submit the block task\n",
    "                future = client.submit(\n",
    "                    mean_downsample_block,\n",
    "                    source_path,\n",
    "                    destination_path,\n",
    "                    block_region,\n",
    "                    destination_coords,\n",
    "                    downsample_factor\n",
    "                )\n",
    "\n",
    "                futures.append(future)\n",
    "\n",
    "                if len(futures) >= max_in_flight:\n",
    "                    completed += 1\n",
    "                    print(f\"completed: {completed}\")\n",
    "                    wait(futures)\n",
    "                    futures = []\n",
    "\n",
    "    if futures:\n",
    "        wait(futures)\n",
    "\n",
    "    print(f\"Finished level {level} in {(time.time() - level_start):.1f}s\")\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    n_workers=n_workers,\n",
    "    threads_per_worker=threads_per_worker,\n",
    "    processes=True,\n",
    "    memory_limit=memory_limit\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "\n",
    "dask_memusage.install(cluster.scheduler, str(log_path))\n",
    "print(f\"Memory logging to: {log_path}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Building OME-Zarr Multi-Resolution Pyramid (Block-Mean)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "compressor = Blosc(\n",
    "    cname=\"zstd\",\n",
    "    clevel=compression_level,\n",
    "    shuffle=Blosc.BITSHUFFLE\n",
    ")\n",
    "\n",
    "pyramid_start = time.time()\n",
    "for level in range(1, pyramid_levels):\n",
    "    build_level(\n",
    "        client,\n",
    "        output_path,\n",
    "        level,\n",
    "        downsample_factor,\n",
    "        target_chunks,\n",
    "        compressor\n",
    "    )\n",
    "\n",
    "print(\"\\nTotal pyramid time: \"\n",
    "      f\"{(time.time() - pyramid_start)/60:.2f} minutes\")\n",
    "\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "317c292c-8283-44a3-a1e6-4337b039da9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Adding OME-Zarr Metadata\n",
      "============================================================\n",
      "DONE\n",
      "\n",
      "Pyramid Summary:\n",
      "------------------------------------------------------------\n",
      "  Level 0: shape=(1651, 2200, 2200), chunks=(64, 64, 64), size=31.96 GB\n",
      "  Level 1: shape=(206, 275, 275), chunks=(64, 64, 64), size=0.06 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_32971/3192047509.py:7: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n",
      "  store = zarr.NestedDirectoryStore(output_path)\n"
     ]
    }
   ],
   "source": [
    "#ADD OME Metadata\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"Adding OME-Zarr Metadata\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "store = zarr.NestedDirectoryStore(output_path)\n",
    "root = zarr.open_group(store, mode=\"a\")  # append mode\n",
    "\n",
    "# Build datasets list\n",
    "datasets = []\n",
    "for level in range(pyramid_levels):\n",
    "    scale_factor = downsample_factor ** level\n",
    "    datasets.append({\n",
    "        'path': str(level),\n",
    "        'coordinateTransformations': [{\n",
    "            'type': 'scale',\n",
    "            'scale': [\n",
    "                float(scale_factor),  # z\n",
    "                float(scale_factor),  # y\n",
    "                float(scale_factor)   # x\n",
    "            ]\n",
    "        }]\n",
    "    })\n",
    "    \n",
    "# Add multiscales metadata\n",
    "root.attrs['multiscales'] = [{\n",
    "    'version': '0.4',\n",
    "    'name': 'pyramid',\n",
    "    'axes': [\n",
    "        {'name': 'z', 'type': 'space', 'unit': 'micrometer'},\n",
    "        {'name': 'y', 'type': 'space', 'unit': 'micrometer'},\n",
    "        {'name': 'x', 'type': 'space', 'unit': 'micrometer'}\n",
    "    ],\n",
    "    'datasets': datasets,\n",
    "    'type': 'mean',  # Downsampling method\n",
    "    'metadata': {\n",
    "        'description': 'Multi-resolution pyramid',\n",
    "        'method': 'block mean downsampling'\n",
    "    }\n",
    "}]\n",
    "print(\"DONE\")\n",
    "print(\"\\nPyramid Summary:\")\n",
    "print(\"-\" * 60)\n",
    "    \n",
    "for level in range(pyramid_levels):\n",
    "    arr = zarr.open(store, mode='r')[str(level)]\n",
    "    size_gb = np.prod(arr.shape) * arr.dtype.itemsize / 1e9\n",
    "    print(f\"  Level {level}: shape={arr.shape}, chunks={arr.chunks}, size={size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be119b9-bd11-43d2-ba4f-61654ca8f912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
