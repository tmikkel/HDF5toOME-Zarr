{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "932c3798-22aa-498f-8b66-449646c659e0",
   "metadata": {},
   "source": [
    "Third version HDF5 -> OME-Zarr Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b22bc11-4442-477d-b983-e7b2461e34f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d43c64a6-6350-4e83-b5e5-f65cb0579d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported succesfully!\n",
      "2.18.7\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import zarr\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask.diagnostics import ProgressBar\n",
    "import numpy as np\n",
    "from numcodecs import Blosc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"All libraries imported succesfully!\")\n",
    "print(zarr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "817a2c10-f80f-403f-9fc1-025a76d8bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion arguments\n",
    "\n",
    "input_path = \"/Users/tobiasschleiss/documents/dtu/thesis/input/brain_2bin_cropSmall.h5\"\n",
    "output_path = \"/Users/tobiasschleiss/Documents/DTU/Thesis/output/test.ome.zarr\"\n",
    "target_chunks = (64, 64, 64)\n",
    "dataset_path = 'exchange/data'\n",
    "max_mem_gb=10\n",
    "safety_factor=0.5\n",
    "pyramid_levels = 5\n",
    "downsample_factor=2\n",
    "compression_level=3\n",
    "target_top_level_mb=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71a5e605-a777-454b-b1b6-a4fc3d57403c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (150, 3768, 2008)\n",
      "  Dtype: float32\n",
      "  Size: 4.23 GB\n",
      "  HDF5 chunks: Contiguous\n"
     ]
    }
   ],
   "source": [
    "# Inspect HDF5 file\n",
    "with h5py.File(input_path, 'r') as f:\n",
    "    if dataset_path not in f:\n",
    "        print(f\"  ERROR: Dataset '{dataset_path}' not found\")\n",
    "        print(f\"  Available paths: {list(f.keys())}\")\n",
    "        \n",
    "    dataset = f[dataset_path]\n",
    "    shape = dataset.shape\n",
    "    dtype = dataset.dtype\n",
    "    h5_chunks = dataset.chunks\n",
    "    data_size_gb = dataset.nbytes / (1024**3)\n",
    "    data_size_mb = dataset.nbytes / (1024**2)\n",
    "    dtype_size = dtype.itemsize\n",
    "        \n",
    "    print(f\"  Shape: {shape}\")\n",
    "    print(f\"  Dtype: {dtype}\")\n",
    "    print(f\"  Size: {data_size_gb:.2f} GB\")\n",
    "    print(f\"  HDF5 chunks: {h5_chunks if h5_chunks else 'Contiguous'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bc38a78-e480-4281-be6b-01abc4e962ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target top level: 10 MB\n",
      "Recommended levels: 4\n",
      "Actual top level: 8.5 MB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Calculate pyramid levels based on target top-level size\"\"\"\n",
    "    \n",
    "# Calculate levels needed\n",
    "levels = 1\n",
    "current_size_mb = data_size_mb\n",
    "    \n",
    "while current_size_mb > target_top_level_mb:\n",
    "    current_size_mb = current_size_mb / (downsample_factor ** 3)\n",
    "    levels += 1\n",
    "    \n",
    "print(f\"Target top level: {target_top_level_mb} MB\")\n",
    "print(f\"Recommended levels: {levels}\")\n",
    "print(f\"Actual top level: {current_size_mb:.1f} MB\")\n",
    "\n",
    "pyramid_levels = levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0ccd6b7-542a-4741-bc30-ec9db9d4b1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Optimal Block Size Calculation\n",
      "============================================================\n",
      "Memory budget: 10.00 GB (using 50%)\n",
      "Available for block: 5.00 GB\n",
      "Actual block size: 3.87 GB\n",
      "============================================================\n",
      "(128, 3768, 2008)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Calculate optimal Z-slab size that:\n",
    "    1. Maximizes memory usage (fewer HDF5 reads)\n",
    "    2. Aligns with target chunks (efficient zarr writes)\n",
    "    3. Stays within memory budget\n",
    "\"\"\"\n",
    "    \n",
    "z, y, x = shape\n",
    "block_z, block_y, block_x = target_chunks\n",
    "    \n",
    "# Available memory given safety factor\n",
    "available_bytes = max_mem_gb * 1e9 * safety_factor\n",
    "    \n",
    "# Calculate maximum amount of Z-planes that fit in memory\n",
    "bytes_per_z_plane = y * x * dtype_size\n",
    "max_z_planes = int(available_bytes / bytes_per_z_plane)\n",
    "\n",
    "\n",
    "if max_z_planes >= block_z:\n",
    "    # Align to target_z for efficient zarr chunking\n",
    "    # Use largest multiple of target_z that fits\n",
    "    optimal_z = (max_z_planes // block_z) * block_z\n",
    "    optimal_z = max(block_z, optimal_z)  # At least one chunk depth\n",
    "    optimal_z = min(optimal_z, z)   # Don't exceed dataset\n",
    "    block_z = optimal_z\n",
    "else:\n",
    "    print(f\"\\nFull Target Z plane ({target_chunks[0]}) too large for memory\")\n",
    "    print(\"Reducing Y axis to fit block in memory\")\n",
    "    \n",
    "    # Calculate max Y that fits with target Z and full X\n",
    "    bytes_per_y_row = block_z * x * dtype_size\n",
    "    max_y_rows = int(available_bytes / bytes_per_y_row)\n",
    "    optimal_y = (max_y_rows // block_y) * block_y \n",
    "    optimal_y = max(block_y, optimal_y)  # At least one chunk depth\n",
    "    if max_y_rows >= y/2+block_y:\n",
    "        optimal_y = int(min(optimal_y, ((y/2)//block_y)*block_y+block_y))   # Don't exceed half of y + target_y\n",
    "    y = optimal_y\n",
    "\n",
    "block_shape = block_z, y, x\n",
    "    \n",
    "# Calculate actual memory usage\n",
    "actual_gb = block_z * y * x * dtype_size / 1e9\n",
    "    \n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Optimal Block Size Calculation\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Memory budget: {max_mem_gb:.2f} GB (using {int(safety_factor*100)}%)\")\n",
    "print(f\"Available for block: {available_bytes/1e9:.2f} GB\")\n",
    "print(f\"Actual block size: {actual_gb:.2f} GB\")\n",
    "print(f\"{'='*60}\")\n",
    "print(block_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2497f95d-ac08-4791-be03-2d881cc96b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: HDF5 -> Level 0 zarr (optimized for sequential reads)\n",
      "  Source shape: (150, 3768, 2008), dtype: float32\n",
      "Processing 2 blocks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_31144/925731418.py:15: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n",
      "  store = zarr.NestedDirectoryStore(output_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Block    2/2 (100.0%) -   0.3 blocks/s - ETA:      0s\n",
      "\n",
      "✓ Level 0 complete in 6.4s\n",
      "  Throughput: 0.71 GB/s\n"
     ]
    }
   ],
   "source": [
    "print(\"Stage 1: HDF5 -> Level 0 zarr (optimized for sequential reads)\")\n",
    "\n",
    "# Open HDF5\n",
    "with h5py.File(input_path, 'r') as f:\n",
    "    dataset = f[dataset_path]\n",
    "    shape = dataset.shape\n",
    "    dtype = dataset.dtype\n",
    "        \n",
    "    print(f\"  Source shape: {shape}, dtype: {dtype}\")\n",
    "\n",
    "    block_z, block_y, block_x = block_shape\n",
    "    z_total, y_total, x_total = shape\n",
    "        \n",
    "    # Setup output zarr store    \n",
    "    store = zarr.NestedDirectoryStore(output_path)\n",
    "    root = zarr.open_group(store=store, mode='w')\n",
    "\n",
    "    # Compressor for all levels\n",
    "    compressor = Blosc(cname='zstd', clevel=compression_level, shuffle=Blosc.BITSHUFFLE)\n",
    "        \n",
    "    level_0 = root.create_dataset(\n",
    "        '0',\n",
    "        shape=shape,\n",
    "        chunks=target_chunks,\n",
    "        dtype=dtype,\n",
    "        compressor=compressor\n",
    "    )\n",
    "        \n",
    "    # Copy in large slabs (efficient for contiguous HDF5)\n",
    "\n",
    "    level0_start = time.time()\n",
    "    block_count = 0\n",
    "        \n",
    "    # Calculate total blocks\n",
    "    total_blocks = (\n",
    "        ((z_total + block_z - 1) // block_z) *\n",
    "        ((y_total + block_y - 1) // block_y)\n",
    "    )\n",
    "        \n",
    "    print(f\"Processing {total_blocks} blocks...\")\n",
    "\n",
    "     # Iterate over blocks\n",
    "    for z_start in range(0, z_total, block_z):\n",
    "        z_end = min(z_start + block_z, z_total)\n",
    "            \n",
    "        for y_start in range(0, y_total, block_y):\n",
    "            y_end = min(y_start + block_y, y_total)\n",
    "                    \n",
    "            block_count += 1\n",
    "                    \n",
    "            # Read block from HDF5\n",
    "            block = dataset[z_start:z_end, y_start:y_end, :]\n",
    "                    \n",
    "            # Write to zarr (zarr will internally chunk to target_chunks)\n",
    "            level_0[z_start:z_end, y_start:y_end, :] = block\n",
    "                    \n",
    "            del block\n",
    "                    \n",
    "            # Progress reporting\n",
    "            if block_count % 2 == 0 or block_count == total_blocks:\n",
    "                elapsed = time.time() - level0_start\n",
    "                rate = block_count / elapsed if elapsed > 0 else 0\n",
    "                eta = (total_blocks - block_count) / rate if rate > 0 else 0\n",
    "                progress = block_count / total_blocks * 100\n",
    "                    \n",
    "                print(f\"  Block {block_count:4d}/{total_blocks} ({progress:5.1f}%) - \"\n",
    "                        f\"{rate:5.1f} blocks/s - ETA: {eta:6.0f}s\")   \n",
    "        \n",
    "    elapsed_level0 = time.time() - level0_start\n",
    "    throughput = (np.prod(shape) * dtype_size / 1e9) / elapsed\n",
    "        \n",
    "    print(f\"\\n✓ Level 0 complete in {elapsed_level0:.1f}s\")\n",
    "    print(f\"  Throughput: {throughput:.2f} GB/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb2ef653-6a03-40f5-bca8-8d772d916b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Source chunks: (64, 64, 64)\n",
      "  Source shape: (150, 3768, 2008)\n"
     ]
    }
   ],
   "source": [
    "# Inspection level_0\n",
    "source = da.from_zarr(output_path, component='0')\n",
    "    \n",
    "print(f\"  Source chunks: {source.chunksize}\")\n",
    "print(f\"  Source shape: {source.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8a3eb80-abf9-4eec-a60a-19aa76a25f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2: Write Multi-Resolution Pyramid from level 0\n",
      "\n",
      "============================================================\n",
      "Building OME-Zarr Multi-Resolution Pyramid\n",
      "\n",
      "============================================================\n",
      "LEVEL 1: Downsampling\n",
      "============================================================\n",
      "Previous shape: (150, 3768, 2008)\n",
      "New shape: (75, 1884, 1004)\n",
      "Source chunks: (64, 64, 64)\n",
      "After coarsen: shape=(75, 1884, 1004), chunks=(32, 32, 32)\n",
      "Rechunked to target chunks: (64, 64, 64)\n",
      "Level size: 0.57 GB\n",
      "Writing level 1...\n",
      "[########################################] | 100% Completed | 4.69 sms\n",
      "\n",
      "============================================================\n",
      "LEVEL 2: Downsampling\n",
      "============================================================\n",
      "Previous shape: (75, 1884, 1004)\n",
      "New shape: (37, 942, 502)\n",
      "Source chunks: (64, 64, 64)\n",
      "After coarsen: shape=(37, 942, 502), chunks=(32, 32, 32)\n",
      "Rechunked to target chunks: (37, 64, 64)\n",
      "Level size: 0.07 GB\n",
      "Writing level 2...\n",
      "[########################################] | 100% Completed | 845.63 ms\n",
      "\n",
      "============================================================\n",
      "LEVEL 3: Downsampling\n",
      "============================================================\n",
      "Previous shape: (37, 942, 502)\n",
      "New shape: (18, 471, 251)\n",
      "Source chunks: (64, 64, 64)\n",
      "After coarsen: shape=(18, 471, 251), chunks=(18, 32, 32)\n",
      "Rechunked to target chunks: (18, 64, 64)\n",
      "Level size: 0.01 GB\n",
      "Writing level 3...\n",
      "[########################################] | 100% Completed | 209.47 ms\n",
      "\n",
      "============================================================\n",
      "Pyramid Complete!\n",
      "============================================================\n",
      "\n",
      "Timing breakdown:\n",
      "  Level 0 write: 6.4s\n",
      "  Pyramid write: 6.3s\n",
      "  Full OME-Zarr multiscale image write: 12.7s\n",
      "  Total runtime: 0:00:12\n"
     ]
    }
   ],
   "source": [
    "print(\"Stage 2: Write Multi-Resolution Pyramid from level 0\")\n",
    "\n",
    "# Configure dask for memory constraints\n",
    "dask.config.set({\n",
    "    'array.chunk-size': f'{int(max_mem_gb * 0.3)}GB',\n",
    "    'distributed.worker.memory.target': 0.7,\n",
    "    'distributed.worker.memory.spill': 0.8,\n",
    "})\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"Building OME-Zarr Multi-Resolution Pyramid\")\n",
    "\n",
    "    \n",
    "# Compressor for all levels\n",
    "compressor = Blosc(cname='zstd', clevel=compression_level, shuffle=Blosc.BITSHUFFLE)\n",
    "    \n",
    "# Load input zarr\n",
    "source = da.from_zarr(output_path, component='0')\n",
    "source_shape = source.shape\n",
    "source_chunks = source.chunksize\n",
    "    \n",
    "# ===== PYRAMID LEVELS: Build each level from previous =====\n",
    "current_shape = source_shape\n",
    "\n",
    "pyramid_start = time.time()\n",
    "    \n",
    "for level in range(1, pyramid_levels):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"LEVEL {level}: Downsampling\")\n",
    "    print(f\"{'='*60}\")\n",
    "        \n",
    "    # Calculate new shape after downsampling\n",
    "    new_shape = tuple(max(1, s // downsample_factor) for s in current_shape)\n",
    "        \n",
    "    print(f\"Previous shape: {current_shape}\")\n",
    "    print(f\"New shape: {new_shape}\")\n",
    "        \n",
    "    # Load previous level\n",
    "    prev_array = da.from_zarr(store, component=str(level - 1))\n",
    "    \n",
    "    # Downsample using coarsen (block mean)\n",
    "    # This is memory efficient and HDD-friendly\n",
    "    downsampled = da.coarsen(\n",
    "        np.mean,\n",
    "        prev_array,\n",
    "        {0: downsample_factor, 1: downsample_factor, 2: downsample_factor},\n",
    "        trim_excess=True\n",
    "    ).astype(prev_array.dtype)\n",
    "        \n",
    "    print(f\"After coarsen: shape={downsampled.shape}, chunks={downsampled.chunksize}\")\n",
    "        \n",
    "    # Adjust target chunks if array is smaller\n",
    "    level_chunks = tuple(min(tc, ns) for tc, ns in zip(target_chunks, new_shape))\n",
    "        \n",
    "    # Rechunk to target (only once, at the end)\n",
    "    downsampled = downsampled.rechunk(level_chunks)\n",
    "        \n",
    "    print(f\"Rechunked to target chunks: {level_chunks}\")\n",
    "        \n",
    "    # Estimate memory\n",
    "    level_size_gb = np.prod(new_shape) * prev_array.dtype.itemsize / 1e9\n",
    "    print(f\"Level size: {level_size_gb:.2f} GB\")\n",
    "        \n",
    "    # Write to zarr with progress bar\n",
    "    print(f\"Writing level {level}...\")\n",
    "        \n",
    "    with ProgressBar():\n",
    "        downsampled.to_zarr(\n",
    "            store,\n",
    "            component=str(level),\n",
    "            compressor=compressor,\n",
    "            dimension_separator='/',  # Nested storage\n",
    "            overwrite=True\n",
    "        )\n",
    "        \n",
    "    # Update current shape\n",
    "    current_shape = new_shape\n",
    "\n",
    "elapsed_pyramid = time.time() - pyramid_start\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Pyramid Complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_seconds = elapsed_pyramid + elapsed_level0\n",
    "\n",
    "print(f\"\\nTiming breakdown:\")\n",
    "print(f\"  Level 0 write: {elapsed_level0:.1f}s\")\n",
    "print(f\"  Pyramid write: {elapsed_pyramid:.1f}s\")\n",
    "print(f\"  Full OME-Zarr multiscale image write: {total_seconds:.1f}s\")\n",
    "print(f\"  Total runtime: {timedelta(seconds=int(total_seconds))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea5983cb-fd66-4c26-a7af-cbc3b0d16d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_31144/3712504397.py:32: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n",
      "  store = zarr.NestedDirectoryStore(output_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BUILDING OME-ZARR PYRAMID (OPTIMIZED FOR NFS/HDD)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "LEVEL 1: Downsampling by 2x\n",
      "================================================================================\n",
      "  Input: (150, 3768, 2008) (chunks: (64, 64, 64))\n",
      "  Output: (75, 1884, 1004) (chunks: (64, 64, 64))\n",
      "  Processing in blocks of ~128x128x128\n",
      "  Total blocks: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Level 1: 100%|████████████████████████████████████████████████| 120/120 [00:07<00:00, 16.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Level 1 complete: 7.3s (0.08 GB/s)\n",
      "\n",
      "================================================================================\n",
      "LEVEL 2: Downsampling by 2x\n",
      "================================================================================\n",
      "  Input: (75, 1884, 1004) (chunks: (64, 64, 64))\n",
      "  Output: (37, 942, 502) (chunks: (37, 64, 64))\n",
      "  Processing in blocks of ~128x128x128\n",
      "  Total blocks: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Level 2: 100%|██████████████████████████████████████████████████| 32/32 [00:00<00:00, 33.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Level 2 complete: 1.0s (0.07 GB/s)\n",
      "\n",
      "================================================================================\n",
      "LEVEL 3: Downsampling by 2x\n",
      "================================================================================\n",
      "  Input: (37, 942, 502) (chunks: (37, 64, 64))\n",
      "  Output: (18, 471, 251) (chunks: (18, 64, 64))\n",
      "  Processing in blocks of ~128x128x128\n",
      "  Total blocks: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Level 3: 100%|████████████████████████████████████████████████████| 8/8 [00:00<00:00, 63.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Level 3 complete: 0.1s (0.07 GB/s)\n",
      "\n",
      "================================================================================\n",
      "✓ PYRAMID COMPLETE: 8.4s (0.1 min)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"BUILDING OME-ZARR PYRAMID (OPTIMIZED FOR NFS/HDD)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "block_size = (128, 128, 128)\n",
    "\n",
    "def downsample_block(block, factor, target_shape):\n",
    "    \"\"\"\n",
    "    Downsample a 3D block by averaging.\n",
    "    Handles edge cases where block size isn't perfectly divisible.\n",
    "    \"\"\"\n",
    "    z, y, x = block.shape\n",
    "    tz, ty, tx = target_shape\n",
    "    \n",
    "    # Trim to multiple of factor\n",
    "    z_trim = (z // factor) * factor\n",
    "    y_trim = (y // factor) * factor\n",
    "    x_trim = (x // factor) * factor\n",
    "    \n",
    "    trimmed = block[:z_trim, :y_trim, :x_trim]\n",
    "    \n",
    "    # Reshape and mean\n",
    "    downsampled = trimmed.reshape(\n",
    "        tz, factor,\n",
    "        ty, factor,\n",
    "        tx, factor\n",
    "    ).mean(axis=(1, 3, 5)).astype(block.dtype)\n",
    "    \n",
    "    return downsampled\n",
    "    \n",
    "compressor = Blosc(cname='zstd', clevel=compression_level, shuffle=Blosc.BITSHUFFLE)\n",
    "store = zarr.NestedDirectoryStore(output_path)\n",
    "root = zarr.open_group(store=store, mode='a')\n",
    "    \n",
    "pyramid_start = time.time()\n",
    "    \n",
    "for level in range(1, pyramid_levels):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"LEVEL {level}: Downsampling by {downsample_factor}x\")\n",
    "    print(f\"{'='*80}\")\n",
    "        \n",
    "    # Open previous level\n",
    "    prev_level = root[str(level - 1)]\n",
    "    prev_shape = prev_level.shape\n",
    "    prev_chunks = prev_level.chunks\n",
    "        \n",
    "    # Calculate new shape\n",
    "    new_shape = tuple(max(1, s // downsample_factor) for s in prev_shape)\n",
    "    level_chunks = tuple(min(tc, ns) for tc, ns in zip(target_chunks, new_shape))\n",
    "        \n",
    "    print(f\"  Input: {prev_shape} (chunks: {prev_chunks})\")\n",
    "    print(f\"  Output: {new_shape} (chunks: {level_chunks})\")\n",
    "        \n",
    "    # Create output dataset\n",
    "    curr_level = root.create_dataset(\n",
    "        str(level),\n",
    "        shape=new_shape,\n",
    "        chunks=level_chunks,\n",
    "        dtype=prev_level.dtype,\n",
    "        compressor=compressor,\n",
    "        overwrite=True\n",
    "    )\n",
    "        \n",
    "    # Sequential block-based downsampling\n",
    "    # Process in large blocks to minimize seeks\n",
    "    level_start = time.time()\n",
    "        \n",
    "    # Calculate processing blocks (larger than chunks for efficiency)\n",
    "    proc_z, proc_y, proc_x = block_size\n",
    "    in_z, in_y, in_x = prev_shape\n",
    "    out_z, out_y, out_x = new_shape\n",
    "        \n",
    "    # Adjust block size for input (must be multiple of downsample_factor)\n",
    "    in_block_z = (proc_z // downsample_factor) * downsample_factor\n",
    "    in_block_y = (proc_y // downsample_factor) * downsample_factor\n",
    "    in_block_x = (proc_x // downsample_factor) * downsample_factor\n",
    "        \n",
    "    total_blocks = (\n",
    "        ((out_z + proc_z - 1) // proc_z) *\n",
    "        ((out_y + proc_y - 1) // proc_y) *\n",
    "        ((out_x + proc_x - 1) // proc_x)\n",
    "    )\n",
    "        \n",
    "    print(f\"  Processing in blocks of ~{proc_z}x{proc_y}x{proc_x}\")\n",
    "    print(f\"  Total blocks: {total_blocks}\")\n",
    "        \n",
    "    block_count = 0\n",
    "        \n",
    "    # Iterate over output blocks\n",
    "    with tqdm(total=total_blocks, desc=f\"Level {level}\") as pbar:\n",
    "        for out_z_start in range(0, out_z, proc_z):\n",
    "            out_z_end = min(out_z_start + proc_z, out_z)\n",
    "            in_z_start = out_z_start * downsample_factor\n",
    "            in_z_end = min(out_z_end * downsample_factor, in_z)\n",
    "                \n",
    "            for out_y_start in range(0, out_y, proc_y):\n",
    "                out_y_end = min(out_y_start + proc_y, out_y)\n",
    "                in_y_start = out_y_start * downsample_factor\n",
    "                in_y_end = min(out_y_end * downsample_factor, in_y)\n",
    "                    \n",
    "                for out_x_start in range(0, out_x, proc_x):\n",
    "                    out_x_end = min(out_x_start + proc_x, out_x)\n",
    "                    in_x_start = out_x_start * downsample_factor\n",
    "                    in_x_end = min(out_x_end * downsample_factor, in_x)\n",
    "                        \n",
    "                    # Read input block\n",
    "                    in_block = prev_level[\n",
    "                        in_z_start:in_z_end,\n",
    "                        in_y_start:in_y_end,\n",
    "                        in_x_start:in_x_end\n",
    "                    ]\n",
    "                        \n",
    "                    # Downsample using reshape + mean\n",
    "                    out_block = downsample_block(\n",
    "                        in_block,\n",
    "                        downsample_factor,\n",
    "                        (out_z_end - out_z_start,\n",
    "                        out_y_end - out_y_start,\n",
    "                        out_x_end - out_x_start)\n",
    "                    )\n",
    "                        \n",
    "                    # Write output block\n",
    "                    curr_level[\n",
    "                        out_z_start:out_z_end,\n",
    "                        out_y_start:out_y_end,\n",
    "                        out_x_start:out_x_end\n",
    "                    ] = out_block\n",
    "                        \n",
    "                    block_count += 1\n",
    "                    pbar.update(1)\n",
    "                        \n",
    "                    del in_block, out_block\n",
    "        \n",
    "    elapsed = time.time() - level_start\n",
    "    size_gb = np.prod(new_shape) * prev_level.dtype.itemsize / 1e9\n",
    "    throughput = size_gb / elapsed\n",
    "        \n",
    "    print(f\"  ✓ Level {level} complete: {elapsed:.1f}s ({throughput:.2f} GB/s)\")\n",
    "    \n",
    "total_time = time.time() - pyramid_start\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ PYRAMID COMPLETE: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "029fdf4e-d4cb-4405-9fd4-c4418c2d9506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 3: Write OME-Zarr Metadata\n",
      "============================================================\n",
      "Adding OME-Zarr Metadata\n",
      "============================================================\n",
      "DONE\n",
      "\n",
      "Pyramid Summary:\n",
      "------------------------------------------------------------\n",
      "  Level 0: shape=(150, 3768, 2008), chunks=(64, 64, 64), size=4.54 GB\n",
      "  Level 1: shape=(75, 1884, 1004), chunks=(64, 64, 64), size=0.57 GB\n",
      "  Level 2: shape=(37, 942, 502), chunks=(37, 64, 64), size=0.07 GB\n",
      "  Level 3: shape=(18, 471, 251), chunks=(18, 64, 64), size=0.01 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Stage 3: Write OME-Zarr Metadata\")\n",
    "\n",
    "# ===== ADD OME-ZARR METADATA =====\n",
    "print(f\"{'='*60}\")\n",
    "print(\"Adding OME-Zarr Metadata\")\n",
    "print(f\"{'='*60}\")\n",
    "    \n",
    "# Build datasets list\n",
    "datasets = []\n",
    "for level in range(pyramid_levels):\n",
    "    scale_factor = downsample_factor ** level\n",
    "    datasets.append({\n",
    "        'path': str(level),\n",
    "        'coordinateTransformations': [{\n",
    "            'type': 'scale',\n",
    "            'scale': [\n",
    "                float(scale_factor),  # z\n",
    "                float(scale_factor),  # y\n",
    "                float(scale_factor)   # x\n",
    "            ]\n",
    "        }]\n",
    "    })\n",
    "    \n",
    "# Add multiscales metadata\n",
    "root.attrs['multiscales'] = [{\n",
    "    'version': '0.4',\n",
    "    'name': 'pyramid',\n",
    "    'axes': [\n",
    "        {'name': 'z', 'type': 'space', 'unit': 'micrometer'},\n",
    "        {'name': 'y', 'type': 'space', 'unit': 'micrometer'},\n",
    "        {'name': 'x', 'type': 'space', 'unit': 'micrometer'}\n",
    "    ],\n",
    "    'datasets': datasets,\n",
    "    'type': 'mean',  # Downsampling method\n",
    "    'metadata': {\n",
    "        'description': 'Multi-resolution pyramid',\n",
    "        'method': 'block mean downsampling'\n",
    "    }\n",
    "}]\n",
    "print(\"DONE\")\n",
    "print(\"\\nPyramid Summary:\")\n",
    "print(\"-\" * 60)\n",
    "    \n",
    "for level in range(pyramid_levels):\n",
    "    arr = zarr.open(store, mode='r')[str(level)]\n",
    "    size_gb = np.prod(arr.shape) * arr.dtype.itemsize / 1e9\n",
    "    print(f\"  Level {level}: shape={arr.shape}, chunks={arr.chunks}, size={size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "930c8826-83bc-4c4c-992d-094b24a283d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OME-Zarr Structure Validation\n",
      "============================================================\n",
      "✓ Root is a group\n",
      "✓ Has multiscales metadata\n",
      "✓ Multiscales defines 4 datasets\n",
      "\n",
      "Dataset Check:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dataset 0: path='0'\n",
      "  ✓ Array exists\n",
      "    Shape: (150, 3768, 2008)\n",
      "    Chunks: (64, 64, 64)\n",
      "    Dtype: float32\n",
      "    Transforms: [{'scale': [1.0, 1.0, 1.0], 'type': 'scale'}]\n",
      "\n",
      "Dataset 1: path='1'\n",
      "  ✓ Array exists\n",
      "    Shape: (75, 1884, 1004)\n",
      "    Chunks: (64, 64, 64)\n",
      "    Dtype: float32\n",
      "    Transforms: [{'scale': [2.0, 2.0, 2.0], 'type': 'scale'}]\n",
      "\n",
      "Dataset 2: path='2'\n",
      "  ✓ Array exists\n",
      "    Shape: (37, 942, 502)\n",
      "    Chunks: (37, 64, 64)\n",
      "    Dtype: float32\n",
      "    Transforms: [{'scale': [4.0, 4.0, 4.0], 'type': 'scale'}]\n",
      "\n",
      "Dataset 3: path='3'\n",
      "  ✓ Array exists\n",
      "    Shape: (18, 471, 251)\n",
      "    Chunks: (18, 64, 64)\n",
      "    Dtype: float32\n",
      "    Transforms: [{'scale': [8.0, 8.0, 8.0], 'type': 'scale'}]\n",
      "\n",
      "============================================================\n",
      "Checking for unexpected arrays...\n",
      "============================================================\n",
      "✓ No unexpected arrays\n",
      "\n",
      "============================================================\n",
      "Full Multiscales Metadata:\n",
      "============================================================\n",
      "[\n",
      "  {\n",
      "    \"axes\": [\n",
      "      {\n",
      "        \"name\": \"z\",\n",
      "        \"type\": \"space\",\n",
      "        \"unit\": \"micrometer\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"y\",\n",
      "        \"type\": \"space\",\n",
      "        \"unit\": \"micrometer\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"x\",\n",
      "        \"type\": \"space\",\n",
      "        \"unit\": \"micrometer\"\n",
      "      }\n",
      "    ],\n",
      "    \"datasets\": [\n",
      "      {\n",
      "        \"coordinateTransformations\": [\n",
      "          {\n",
      "            \"scale\": [\n",
      "              1.0,\n",
      "              1.0,\n",
      "              1.0\n",
      "            ],\n",
      "            \"type\": \"scale\"\n",
      "          }\n",
      "        ],\n",
      "        \"path\": \"0\"\n",
      "      },\n",
      "      {\n",
      "        \"coordinateTransformations\": [\n",
      "          {\n",
      "            \"scale\": [\n",
      "              2.0,\n",
      "              2.0,\n",
      "              2.0\n",
      "            ],\n",
      "            \"type\": \"scale\"\n",
      "          }\n",
      "        ],\n",
      "        \"path\": \"1\"\n",
      "      },\n",
      "      {\n",
      "        \"coordinateTransformations\": [\n",
      "          {\n",
      "            \"scale\": [\n",
      "              4.0,\n",
      "              4.0,\n",
      "              4.0\n",
      "            ],\n",
      "            \"type\": \"scale\"\n",
      "          }\n",
      "        ],\n",
      "        \"path\": \"2\"\n",
      "      },\n",
      "      {\n",
      "        \"coordinateTransformations\": [\n",
      "          {\n",
      "            \"scale\": [\n",
      "              8.0,\n",
      "              8.0,\n",
      "              8.0\n",
      "            ],\n",
      "            \"type\": \"scale\"\n",
      "          }\n",
      "        ],\n",
      "        \"path\": \"3\"\n",
      "      }\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "      \"description\": \"Multi-resolution pyramid\",\n",
      "      \"method\": \"block mean downsampling\"\n",
      "    },\n",
      "    \"name\": \"pyramid\",\n",
      "    \"type\": \"mean\",\n",
      "    \"version\": \"0.4\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zarr\n",
    "import json\n",
    "\n",
    "def validate_ome_zarr_structure(path):\n",
    "    \"\"\"Check if OME-Zarr structure is correct\"\"\"\n",
    "    \n",
    "    root = zarr.open(path, mode='r')\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"OME-Zarr Structure Validation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check root is a group\n",
    "    if not isinstance(root, zarr.hierarchy.Group):\n",
    "        print(\"❌ Root is not a group!\")\n",
    "        return False\n",
    "    \n",
    "    print(\"✓ Root is a group\")\n",
    "    \n",
    "    # Check multiscales metadata\n",
    "    if 'multiscales' not in root.attrs:\n",
    "        print(\"❌ Missing 'multiscales' attribute!\")\n",
    "        return False\n",
    "    \n",
    "    print(\"✓ Has multiscales metadata\")\n",
    "    \n",
    "    multiscales = root.attrs['multiscales']\n",
    "    if not isinstance(multiscales, list) or len(multiscales) == 0:\n",
    "        print(\"❌ Multiscales is not a list or is empty!\")\n",
    "        return False\n",
    "    \n",
    "    ms = multiscales[0]\n",
    "    datasets = ms.get('datasets', [])\n",
    "    \n",
    "    print(f\"✓ Multiscales defines {len(datasets)} datasets\")\n",
    "    \n",
    "    # Check each dataset exists\n",
    "    print(\"\\nDataset Check:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, ds in enumerate(datasets):\n",
    "        path_name = ds.get('path')\n",
    "        print(f\"\\nDataset {i}: path='{path_name}'\")\n",
    "        \n",
    "        if path_name not in root:\n",
    "            print(f\"  ❌ Array '{path_name}' not found in root!\")\n",
    "            return False\n",
    "        \n",
    "        arr = root[path_name]\n",
    "        \n",
    "        if not isinstance(arr, zarr.core.Array):\n",
    "            print(f\"  ❌ '{path_name}' is not an array!\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"  ✓ Array exists\")\n",
    "        print(f\"    Shape: {arr.shape}\")\n",
    "        print(f\"    Chunks: {arr.chunks}\")\n",
    "        print(f\"    Dtype: {arr.dtype}\")\n",
    "        \n",
    "        # Check coordinate transformations\n",
    "        if 'coordinateTransformations' in ds:\n",
    "            transforms = ds['coordinateTransformations']\n",
    "            print(f\"    Transforms: {transforms}\")\n",
    "    \n",
    "    # Check for extra arrays\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Checking for unexpected arrays...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    expected_paths = {ds['path'] for ds in datasets}\n",
    "    actual_paths = set(root.array_keys())\n",
    "    \n",
    "    extra = actual_paths - expected_paths\n",
    "    missing = expected_paths - actual_paths\n",
    "    \n",
    "    if extra:\n",
    "        print(f\"⚠️  Found unexpected arrays: {extra}\")\n",
    "        print(\"   These might cause validator confusion!\")\n",
    "    else:\n",
    "        print(\"✓ No unexpected arrays\")\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"❌ Missing expected arrays: {missing}\")\n",
    "        return False\n",
    "    \n",
    "    # Print full metadata\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Full Multiscales Metadata:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(json.dumps(multiscales, indent=2))\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run validation\n",
    "validate_ome_zarr_structure(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
