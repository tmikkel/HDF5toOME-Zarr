{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "932c3798-22aa-498f-8b66-449646c659e0",
   "metadata": {},
   "source": [
    "Third version HDF5 -> OME-Zarr Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43c64a6-6350-4e83-b5e5-f65cb0579d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported succesfully!\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import zarr\n",
    "import time\n",
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "from numcodecs import Blosc\n",
    "from pathlib import Path\n",
    "from ome_zarr.io import parse_url\n",
    "from ome_zarr.writer import write_multiscale\n",
    "\n",
    "print(\"All libraries imported succesfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "817a2c10-f80f-403f-9fc1-025a76d8bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion arguments\n",
    "\n",
    "input_path = \"/Users/tobiasschleiss/documents/dtu/thesis/input/brain_2bin_cropSmall.h5\"\n",
    "output_path = \"/Users/tobiasschleiss/Documents/DTU/Thesis/output/output.ome.zarr\"\n",
    "target_chunks = (64, 64, 64)\n",
    "dataset_path = 'exchange/data'\n",
    "temp_chunk_size=(64, 512, 512)\n",
    "max_mem_gb=12.4\n",
    "pyramid_levels = 5\n",
    "downsample_factor=2\n",
    "compression_level=3\n",
    "target_top_level_mb=10\n",
    "\n",
    "n_workers = 2\n",
    "worker_mem = 4 # memory per wroker in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a5e605-a777-454b-b1b6-a4fc3d57403c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (150, 3768, 2008)\n",
      "  Dtype: float32\n",
      "  Size: 4.23 GB\n",
      "  HDF5 chunks: Contiguous\n"
     ]
    }
   ],
   "source": [
    "# Inspect HDF5 file\n",
    "with h5py.File(input_path, 'r') as f:\n",
    "    if dataset_path not in f:\n",
    "        print(f\"  ERROR: Dataset '{dataset_path}' not found\")\n",
    "        print(f\"  Available paths: {list(f.keys())}\")\n",
    "        \n",
    "    dataset = f[dataset_path]\n",
    "    shape = dataset.shape\n",
    "    dtype = dataset.dtype\n",
    "    h5_chunks = dataset.chunks\n",
    "    data_size_gb = dataset.nbytes / (1024**3)\n",
    "    data_size_mb = dataset.nbytes / (1024**2)\n",
    "    dtype_size = dtype.itemsize\n",
    "        \n",
    "    print(f\"  Shape: {shape}\")\n",
    "    print(f\"  Dtype: {dtype}\")\n",
    "    print(f\"  Size: {data_size_gb:.2f} GB\")\n",
    "    print(f\"  HDF5 chunks: {h5_chunks if h5_chunks else 'Contiguous'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc38a78-e480-4281-be6b-01abc4e962ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base size: 4329.4 MB\n",
      "Target top level: 10 MB\n",
      "Recommended levels: 4\n",
      "Actual top level: 8.5 MB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Calculate pyramid levels based on target top-level size\"\"\"\n",
    "    \n",
    "# Calculate levels needed\n",
    "levels = 1\n",
    "current_size_mb = data_size_mb\n",
    "    \n",
    "while current_size_mb > target_top_level_mb:\n",
    "    current_size_mb = current_size_mb / (downsample_factor ** 3)\n",
    "    levels += 1\n",
    "\n",
    "print(f\"Target top level: {target_top_level_mb} MB\")\n",
    "print(f\"Recommended levels: {levels}\")\n",
    "print(f\"Actual top level: {current_size_mb:.1f} MB\")\n",
    "\n",
    "pyramid_levels = levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2497f95d-ac08-4791-be03-2d881cc96b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: HDF5 -> Temporary Zarr (optimized for sequential reads)\n",
      "/Users/tobiasschleiss/Documents/DTU/Thesis/output/temp_rechunk.zarr\n",
      "  Source shape: (150, 3768, 2008), dtype: float32\n",
      "  Reading Z planes 0-64\n",
      "  Reading Z planes 64-128\n",
      "  Reading Z planes 128-150\n",
      "\n",
      "Timing breakdown:\n",
      "  Writing: 3.1s\n"
     ]
    }
   ],
   "source": [
    "print(\"Stage 1: HDF5 -> Temporary Zarr (optimized for sequential reads)\")\n",
    "    \n",
    "temp_path = Path(output_path).parent / \"temp_rechunk.zarr\"\n",
    "print(temp_path)\n",
    "    \n",
    "# Open HDF5\n",
    "with h5py.File(input_path, 'r') as f:\n",
    "    dataset = f[dataset_path]\n",
    "    shape = dataset.shape\n",
    "    dtype = dataset.dtype\n",
    "        \n",
    "    print(f\"  Source shape: {shape}, dtype: {dtype}\")\n",
    "        \n",
    "    # Create temporary zarr with chunks optimized for reading from contiguous HDF5\n",
    "    temp_store = zarr.DirectoryStore(temp_path)\n",
    "    temp_root = zarr.open_group(temp_store, mode='w')\n",
    "        \n",
    "    temp_array = temp_root.create_dataset(\n",
    "        'data',\n",
    "        shape=shape,\n",
    "        chunks=temp_chunk_size,  # Large Z-slabs\n",
    "        dtype=dtype,\n",
    "        compressor=Blosc(cname='lz4', clevel=1)  # Fast compression for temp\n",
    "    )\n",
    "        \n",
    "    # Copy in large slabs (efficient for contiguous HDF5)\n",
    "\n",
    "    temp_start = time.time()\n",
    "    \n",
    "    slab_size = temp_chunk_size[0]\n",
    "    for z_start in range(0, shape[0], slab_size):\n",
    "        z_end = min(z_start + slab_size, shape[0])\n",
    "        print(f\"  Reading Z planes {z_start}-{z_end}\")\n",
    "            \n",
    "        slab = dataset[z_start:z_end, :, :]\n",
    "        temp_array[z_start:z_end, :, :] = slab\n",
    "        del slab\n",
    "\n",
    "    temp_time = time.time() - temp_start\n",
    "\n",
    "    print(f\"\\nTiming breakdown:\")\n",
    "    print(f\"  Writing: {temp_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a3eb80-abf9-4eec-a60a-19aa76a25f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Building OME-Zarr Multi-Resolution Pyramid\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "LEVEL 0: Rechunking to target chunks\n",
      "============================================================\n",
      "Input shape: (150, 3768, 2008)\n",
      "Input chunks: (64, 512, 512)\n",
      "Target chunks: (64, 64, 64)\n",
      "Estimated memory usage: 0.68 GB\n",
      "Writing level 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/zbsy0qhn18sfk0qmktm5qs3h0000gn/T/ipykernel_9435/4195199669.py:13: FutureWarning: The NestedDirectoryStore is deprecated and will be removed in a Zarr-Python version 3, see https://github.com/zarr-developers/zarr-python/issues/1274 for more information.\n",
      "  store = zarr.NestedDirectoryStore(output_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Level 0 complete: shape=(150, 3768, 2008), chunks=(64, 64, 64)\n",
      "\n",
      "============================================================\n",
      "LEVEL 1: Downsampling\n",
      "============================================================\n",
      "Previous shape: (150, 3768, 2008)\n",
      "New shape: (75, 1884, 1004)\n",
      "Reading with chunks: (128, 128, 128)\n",
      "Target chunks: (64, 64, 64)\n",
      "Estimated memory: 0.08 GB\n",
      "Writing level 1...\n",
      "✓ Level 1 complete: shape=(75, 1884, 1004), chunks=(64, 64, 64)\n",
      "\n",
      "============================================================\n",
      "LEVEL 2: Downsampling\n",
      "============================================================\n",
      "Previous shape: (75, 1884, 1004)\n",
      "New shape: (37, 942, 502)\n",
      "Reading with chunks: (75, 128, 128)\n",
      "Target chunks: (37, 64, 64)\n",
      "Estimated memory: 0.05 GB\n",
      "Writing level 2...\n",
      "✓ Level 2 complete: shape=(37, 942, 502), chunks=(37, 64, 64)\n",
      "\n",
      "============================================================\n",
      "LEVEL 3: Downsampling\n",
      "============================================================\n",
      "Previous shape: (37, 942, 502)\n",
      "New shape: (18, 471, 251)\n",
      "Reading with chunks: (37, 128, 128)\n",
      "Target chunks: (18, 64, 64)\n",
      "Estimated memory: 0.02 GB\n",
      "Writing level 3...\n",
      "✓ Level 3 complete: shape=(18, 471, 251), chunks=(18, 64, 64)\n",
      "\n",
      "============================================================\n",
      "Adding OME-Zarr Metadata\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Pyramid Complete!\n",
      "============================================================\n",
      "\n",
      "Output: /Users/tobiasschleiss/Documents/DTU/Thesis/output/output.ome.zarr\n",
      "Number of levels: 4\n",
      "\n",
      "Pyramid Summary:\n",
      "------------------------------------------------------------\n",
      "  Level 0: shape=(150, 3768, 2008), chunks=(64, 64, 64), size=4.54 GB\n",
      "  Level 1: shape=(75, 1884, 1004), chunks=(64, 64, 64), size=0.57 GB\n",
      "  Level 2: shape=(37, 942, 502), chunks=(37, 64, 64), size=0.07 GB\n",
      "  Level 3: shape=(18, 471, 251), chunks=(18, 64, 64), size=0.01 GB\n",
      "Temporary files cleaned up\n"
     ]
    }
   ],
   "source": [
    "# Configure dask for memory constraints\n",
    "dask.config.set({\n",
    "    'array.chunk-size': f'{int(max_mem_gb * 0.3)}GB',\n",
    "    'distributed.worker.memory.target': 0.7,\n",
    "    'distributed.worker.memory.spill': 0.8,\n",
    "})\n",
    "    \n",
    "print(\"=\" * 60)\n",
    "print(\"Building OME-Zarr Multi-Resolution Pyramid\")\n",
    "print(\"=\" * 60)\n",
    "    \n",
    "# Setup output zarr store    \n",
    "store = zarr.NestedDirectoryStore(output_path)\n",
    "root = zarr.open_group(store=store, mode='w')\n",
    "    \n",
    "# Compressor for all levels\n",
    "compressor = Blosc(cname='zstd', clevel=compression_level, shuffle=Blosc.BITSHUFFLE)\n",
    "    \n",
    "# ===== LEVEL 0: Rechunk from (64, 512, 512) to (64, 64, 64) =====\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LEVEL 0: Rechunking to target chunks\")\n",
    "print(f\"{'='*60}\")\n",
    "    \n",
    "# Load input zarr\n",
    "source = da.from_zarr(temp_path, component='data')\n",
    "source_shape = source.shape\n",
    "source_chunks = source.chunksize\n",
    "    \n",
    "print(f\"Input shape: {source_shape}\")\n",
    "print(f\"Input chunks: {source_chunks}\")\n",
    "print(f\"Target chunks: {target_chunks}\")\n",
    "    \n",
    "# Calculate memory requirements\n",
    "source_chunk_bytes = np.prod(source_chunks) * source.dtype.itemsize\n",
    "target_chunk_bytes = np.prod(target_chunks) * source.dtype.itemsize\n",
    "    \n",
    "# Estimate memory: dask typically needs ~10 chunks in memory during rechunking\n",
    "estimated_mem_gb = (source_chunk_bytes * 10 + target_chunk_bytes * 10) / 1e9\n",
    "print(f\"Estimated memory usage: {estimated_mem_gb:.2f} GB\")\n",
    "    \n",
    "if estimated_mem_gb > max_mem_gb * 0.8:\n",
    "    print(f\"WARNING: May approach memory limit!\")\n",
    "    \n",
    "# Rechunk to target\n",
    "level_0 = source.rechunk(target_chunks)\n",
    "    \n",
    "# Write level 0\n",
    "print(\"Writing level 0...\")\n",
    "level_0.to_zarr(\n",
    "    store,\n",
    "    component='0',\n",
    "    compressor=compressor,\n",
    "    overwrite=True\n",
    ")\n",
    "    \n",
    "print(f\"✓ Level 0 complete: shape={source_shape}, chunks={target_chunks}\")\n",
    "    \n",
    "# ===== PYRAMID LEVELS: Build each level from previous =====\n",
    "current_shape = source_shape\n",
    "    \n",
    "for level in range(1, pyramid_levels):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"LEVEL {level}: Downsampling\")\n",
    "    print(f\"{'='*60}\")\n",
    "        \n",
    "    # Calculate new shape after downsampling\n",
    "    new_shape = tuple(max(1, s // downsample_factor) for s in current_shape)\n",
    "        \n",
    "    print(f\"Previous shape: {current_shape}\")\n",
    "    print(f\"New shape: {new_shape}\")\n",
    "        \n",
    "    # Load previous level\n",
    "    prev_array = da.from_zarr(store, component=str(level - 1))\n",
    "        \n",
    "    # Calculate optimal chunk size for reading previous level\n",
    "    # We want chunks that align well with both source and target\n",
    "    read_chunks = tuple(\n",
    "        min(c * downsample_factor, s) \n",
    "        for c, s in zip(target_chunks, current_shape)\n",
    "    )\n",
    "        \n",
    "    # Rechunk previous level for efficient downsampling\n",
    "    prev_array = prev_array.rechunk(read_chunks)\n",
    "        \n",
    "    print(f\"Reading with chunks: {read_chunks}\")\n",
    "    \n",
    "    # Downsample using coarsen (block mean)\n",
    "    # This is memory efficient and HDD-friendly\n",
    "    downsampled = da.coarsen(\n",
    "        np.mean,\n",
    "        prev_array,\n",
    "        {0: downsample_factor, 1: downsample_factor, 2: downsample_factor},\n",
    "        trim_excess=True\n",
    "    ).astype(prev_array.dtype)\n",
    "        \n",
    "    # Rechunk to target chunks\n",
    "    # Adjust chunk size if array is smaller than target chunks\n",
    "    level_chunks = tuple(min(tc, ns) for tc, ns in zip(target_chunks, new_shape))\n",
    "    downsampled = downsampled.rechunk(level_chunks)\n",
    "        \n",
    "    print(f\"Target chunks: {level_chunks}\")\n",
    "        \n",
    "    # Calculate memory for this level\n",
    "    level_mem_gb = np.prod(read_chunks) * prev_array.dtype.itemsize * 10 / 1e9\n",
    "    print(f\"Estimated memory: {level_mem_gb:.2f} GB\")\n",
    "        \n",
    "    # Write to zarr\n",
    "    print(f\"Writing level {level}...\")\n",
    "    downsampled.to_zarr(\n",
    "        store,\n",
    "        component=str(level),\n",
    "        compressor=compressor,\n",
    "        overwrite=True\n",
    "    )\n",
    "        \n",
    "    print(f\"✓ Level {level} complete: shape={new_shape}, chunks={level_chunks}\")\n",
    "        \n",
    "    # Update current shape for next iteration\n",
    "    current_shape = new_shape\n",
    "    \n",
    "# ===== ADD OME-ZARR METADATA =====\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Adding OME-Zarr Metadata\")\n",
    "print(f\"{'='*60}\")\n",
    "    \n",
    "# Build datasets list\n",
    "datasets = []\n",
    "for level in range(pyramid_levels):\n",
    "    scale_factor = downsample_factor ** level\n",
    "    datasets.append({\n",
    "        'path': str(level),\n",
    "        'coordinateTransformations': [{\n",
    "            'type': 'scale',\n",
    "            'scale': [\n",
    "                float(scale_factor),  # z\n",
    "                float(scale_factor),  # y\n",
    "                float(scale_factor)   # x\n",
    "            ]\n",
    "        }]\n",
    "    })\n",
    "    \n",
    "# Add multiscales metadata\n",
    "root.attrs['multiscales'] = [{\n",
    "    'version': '0.4',\n",
    "    'name': 'pyramid',\n",
    "    'axes': [\n",
    "        {'name': 'z', 'type': 'space', 'unit': 'micrometer'},\n",
    "        {'name': 'y', 'type': 'space', 'unit': 'micrometer'},\n",
    "        {'name': 'x', 'type': 'space', 'unit': 'micrometer'}\n",
    "    ],\n",
    "    'datasets': datasets,\n",
    "    'type': 'mean',  # Downsampling method\n",
    "    'metadata': {\n",
    "        'description': 'Multi-resolution pyramid',\n",
    "        'method': 'block mean downsampling'\n",
    "    }\n",
    "}]\n",
    "    \n",
    "# Add optional OMERO metadata for visualization\n",
    "root.attrs['omero'] = {\n",
    "    'version': '0.4',\n",
    "    'channels': [{\n",
    "        'color': 'FFFFFF',\n",
    "        'window': {'start': 0, 'end': 65535, 'min': 0, 'max': 65535},\n",
    "        'label': 'Channel_0',\n",
    "        'active': True\n",
    "    }]\n",
    "}\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Pyramid Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOutput: {output_path}\")\n",
    "print(f\"Number of levels: {pyramid_levels}\")\n",
    "print(\"\\nPyramid Summary:\")\n",
    "print(\"-\" * 60)\n",
    "    \n",
    "for level in range(pyramid_levels):\n",
    "    arr = zarr.open(store, mode='r')[str(level)]\n",
    "    size_gb = np.prod(arr.shape) * arr.dtype.itemsize / 1e9\n",
    "    print(f\"  Level {level}: shape={arr.shape}, chunks={arr.chunks}, size={size_gb:.2f} GB\")\n",
    "    \n",
    "# Cleanup temp\n",
    "import shutil\n",
    "shutil.rmtree(temp_path)\n",
    "print(\"Temporary files cleaned up\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2ef653-6a03-40f5-bca8-8d772d916b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Source chunks: (64, 512, 512)\n",
      "  Source shape: (150, 3768, 2008)\n"
     ]
    }
   ],
   "source": [
    "# Inspection of temp data\n",
    "source = da.from_zarr(\"/Users/tobiasschleiss/Documents/DTU/Thesis/output/temp_rechunk.zarr\", component='data')\n",
    "    \n",
    "print(f\"  Source chunks: {source.chunksize}\")\n",
    "print(f\"  Source shape: {source.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1927896f-b3e8-49ff-8b4f-27cafc1b4db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All arrays in root: ['0', '1']\n",
      "All groups in root: []\n"
     ]
    }
   ],
   "source": [
    "# Check for unexpected arrays\n",
    "root = zarr.open('/Users/tobiasschleiss/Documents/DTU/Thesis/output/output.ome.zarr', mode='r')\n",
    "print(\"All arrays in root:\", list(root.array_keys()))\n",
    "print(\"All groups in root:\", list(root.group_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "930c8826-83bc-4c4c-992d-094b24a283d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OME-Zarr Structure Validation\n",
      "============================================================\n",
      "✓ Root is a group\n",
      "✓ Has multiscales metadata\n",
      "✓ Multiscales defines 2 datasets\n",
      "\n",
      "Dataset Check:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dataset 0: path='0'\n",
      "  ✓ Array exists\n",
      "    Shape: (150, 3768, 2008)\n",
      "    Chunks: (64, 64, 64)\n",
      "    Dtype: float32\n",
      "    Transforms: [{'scale': [1.0, 1.0, 1.0], 'type': 'scale'}]\n",
      "\n",
      "Dataset 1: path='1'\n",
      "  ✓ Array exists\n",
      "    Shape: (75, 1884, 1004)\n",
      "    Chunks: (64, 64, 64)\n",
      "    Dtype: float32\n",
      "    Transforms: [{'scale': [2.0, 2.0, 2.0], 'type': 'scale'}]\n",
      "\n",
      "============================================================\n",
      "Checking for unexpected arrays...\n",
      "============================================================\n",
      "✓ No unexpected arrays\n",
      "\n",
      "============================================================\n",
      "Full Multiscales Metadata:\n",
      "============================================================\n",
      "[\n",
      "  {\n",
      "    \"axes\": [\n",
      "      {\n",
      "        \"name\": \"z\",\n",
      "        \"type\": \"space\",\n",
      "        \"unit\": \"micrometer\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"y\",\n",
      "        \"type\": \"space\",\n",
      "        \"unit\": \"micrometer\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"x\",\n",
      "        \"type\": \"space\",\n",
      "        \"unit\": \"micrometer\"\n",
      "      }\n",
      "    ],\n",
      "    \"datasets\": [\n",
      "      {\n",
      "        \"coordinateTransformations\": [\n",
      "          {\n",
      "            \"scale\": [\n",
      "              1.0,\n",
      "              1.0,\n",
      "              1.0\n",
      "            ],\n",
      "            \"type\": \"scale\"\n",
      "          }\n",
      "        ],\n",
      "        \"path\": \"0\"\n",
      "      },\n",
      "      {\n",
      "        \"coordinateTransformations\": [\n",
      "          {\n",
      "            \"scale\": [\n",
      "              2.0,\n",
      "              2.0,\n",
      "              2.0\n",
      "            ],\n",
      "            \"type\": \"scale\"\n",
      "          }\n",
      "        ],\n",
      "        \"path\": \"1\"\n",
      "      }\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "      \"description\": \"Multi-resolution pyramid\",\n",
      "      \"method\": \"block mean downsampling\"\n",
      "    },\n",
      "    \"name\": \"pyramid\",\n",
      "    \"type\": \"mean\",\n",
      "    \"version\": \"0.4\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zarr\n",
    "import json\n",
    "\n",
    "def validate_ome_zarr_structure(path):\n",
    "    \"\"\"Check if OME-Zarr structure is correct\"\"\"\n",
    "    \n",
    "    root = zarr.open(path, mode='r')\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"OME-Zarr Structure Validation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check root is a group\n",
    "    if not isinstance(root, zarr.hierarchy.Group):\n",
    "        print(\"❌ Root is not a group!\")\n",
    "        return False\n",
    "    \n",
    "    print(\"✓ Root is a group\")\n",
    "    \n",
    "    # Check multiscales metadata\n",
    "    if 'multiscales' not in root.attrs:\n",
    "        print(\"❌ Missing 'multiscales' attribute!\")\n",
    "        return False\n",
    "    \n",
    "    print(\"✓ Has multiscales metadata\")\n",
    "    \n",
    "    multiscales = root.attrs['multiscales']\n",
    "    if not isinstance(multiscales, list) or len(multiscales) == 0:\n",
    "        print(\"❌ Multiscales is not a list or is empty!\")\n",
    "        return False\n",
    "    \n",
    "    ms = multiscales[0]\n",
    "    datasets = ms.get('datasets', [])\n",
    "    \n",
    "    print(f\"✓ Multiscales defines {len(datasets)} datasets\")\n",
    "    \n",
    "    # Check each dataset exists\n",
    "    print(\"\\nDataset Check:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, ds in enumerate(datasets):\n",
    "        path_name = ds.get('path')\n",
    "        print(f\"\\nDataset {i}: path='{path_name}'\")\n",
    "        \n",
    "        if path_name not in root:\n",
    "            print(f\"  ❌ Array '{path_name}' not found in root!\")\n",
    "            return False\n",
    "        \n",
    "        arr = root[path_name]\n",
    "        \n",
    "        if not isinstance(arr, zarr.core.Array):\n",
    "            print(f\"  ❌ '{path_name}' is not an array!\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"  ✓ Array exists\")\n",
    "        print(f\"    Shape: {arr.shape}\")\n",
    "        print(f\"    Chunks: {arr.chunks}\")\n",
    "        print(f\"    Dtype: {arr.dtype}\")\n",
    "        \n",
    "        # Check coordinate transformations\n",
    "        if 'coordinateTransformations' in ds:\n",
    "            transforms = ds['coordinateTransformations']\n",
    "            print(f\"    Transforms: {transforms}\")\n",
    "    \n",
    "    # Check for extra arrays\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Checking for unexpected arrays...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    expected_paths = {ds['path'] for ds in datasets}\n",
    "    actual_paths = set(root.array_keys())\n",
    "    \n",
    "    extra = actual_paths - expected_paths\n",
    "    missing = expected_paths - actual_paths\n",
    "    \n",
    "    if extra:\n",
    "        print(f\"⚠️  Found unexpected arrays: {extra}\")\n",
    "        print(\"   These might cause validator confusion!\")\n",
    "    else:\n",
    "        print(\"✓ No unexpected arrays\")\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"❌ Missing expected arrays: {missing}\")\n",
    "        return False\n",
    "    \n",
    "    # Print full metadata\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Full Multiscales Metadata:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(json.dumps(multiscales, indent=2))\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run validation\n",
    "validate_ome_zarr_structure('/Users/tobiasschleiss/Documents/DTU/Thesis/output/output.ome.zarr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
