{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "932c3798-22aa-498f-8b66-449646c659e0",
   "metadata": {},
   "source": [
    "Third version HDF5 -> OME-Zarr Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43c64a6-6350-4e83-b5e5-f65cb0579d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported succesfully!\n",
      "3.0.0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import zarr\n",
    "from zarr.storage import LocalStore\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "from zarr.codecs import BloscCodec, BytesCodec\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"All libraries imported succesfully!\")\n",
    "print(zarr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "817a2c10-f80f-403f-9fc1-025a76d8bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion arguments\n",
    "\n",
    "input_path = \"/Users/tobiasschleiss/documents/dtu/thesis/input/brain_2bin_cropSmall.h5\"\n",
    "output_path = \"/Users/tobiasschleiss/Documents/DTU/Thesis/output/output.ome.zarr\"\n",
    "target_chunks = (64, 64, 64)\n",
    "dataset_path = 'exchange/data'\n",
    "max_mem_gb=10\n",
    "pyramid_levels = 5\n",
    "downsample_factor=2\n",
    "compression_level=3\n",
    "target_top_level_mb=10\n",
    "\n",
    "n_workers = 2\n",
    "worker_mem = 4 # memory per wroker in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a5e605-a777-454b-b1b6-a4fc3d57403c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (150, 3768, 2008)\n",
      "  Dtype: float32\n",
      "  Size: 4.23 GB\n",
      "  HDF5 chunks: Contiguous\n"
     ]
    }
   ],
   "source": [
    "# Inspect HDF5 file\n",
    "with h5py.File(input_path, 'r') as f:\n",
    "    if dataset_path not in f:\n",
    "        print(f\"  ERROR: Dataset '{dataset_path}' not found\")\n",
    "        print(f\"  Available paths: {list(f.keys())}\")\n",
    "        \n",
    "    dataset = f[dataset_path]\n",
    "    shape = dataset.shape\n",
    "    dtype = dataset.dtype\n",
    "    h5_chunks = dataset.chunks\n",
    "    data_size_gb = dataset.nbytes / (1024**3)\n",
    "    data_size_mb = dataset.nbytes / (1024**2)\n",
    "    dtype_size = dtype.itemsize\n",
    "        \n",
    "    print(f\"  Shape: {shape}\")\n",
    "    print(f\"  Dtype: {dtype}\")\n",
    "    print(f\"  Size: {data_size_gb:.2f} GB\")\n",
    "    print(f\"  HDF5 chunks: {h5_chunks if h5_chunks else 'Contiguous'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc38a78-e480-4281-be6b-01abc4e962ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target top level: 10 MB\n",
      "Recommended levels: 4\n",
      "Actual top level: 8.5 MB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Calculate pyramid levels based on target top-level size\"\"\"\n",
    "    \n",
    "# Calculate levels needed\n",
    "levels = 1\n",
    "current_size_mb = data_size_mb\n",
    "    \n",
    "while current_size_mb > target_top_level_mb:\n",
    "    current_size_mb = current_size_mb / (downsample_factor ** 3)\n",
    "    levels += 1\n",
    "\n",
    "print(f\"Target top level: {target_top_level_mb} MB\")\n",
    "print(f\"Recommended levels: {levels}\")\n",
    "print(f\"Actual top level: {current_size_mb:.1f} MB\")\n",
    "\n",
    "pyramid_levels = levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2497f95d-ac08-4791-be03-2d881cc96b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: HDF5 -> Level 0 zarr (optimized for sequential reads)\n",
      "  Source shape: (150, 3768, 2008), dtype: float32\n",
      "  Reading Z planes 0-64\n",
      "  Reading Z planes 64-128\n",
      "  Reading Z planes 128-150\n",
      "\n",
      "Timing breakdown:\n",
      "  Level 0 write: 10.3s\n"
     ]
    }
   ],
   "source": [
    "print(\"Stage 1: HDF5 -> Level 0 zarr (optimized for sequential reads)\")\n",
    "\n",
    "# Open HDF5\n",
    "with h5py.File(input_path, 'r') as f:\n",
    "    dataset = f[dataset_path]\n",
    "    shape = dataset.shape\n",
    "    dtype = dataset.dtype\n",
    "        \n",
    "    print(f\"  Source shape: {shape}, dtype: {dtype}\")\n",
    "        \n",
    "    # Setup output zarr store    \n",
    "    # store = zarr.NestedDirectoryStore(output_path) # NestedDirectoryStore deprecated\n",
    "    # root = zarr.open_group(store=store, mode='w')\n",
    "\n",
    "    store = LocalStore(output_path)\n",
    "\n",
    "    root = zarr.group(store=store, overwrite=True)\n",
    "\n",
    "    # Compressor for all levels\n",
    "    compressor = BloscCodec(\n",
    "    cname='zstd',\n",
    "    clevel=compression_level,\n",
    "    shuffle='bitshuffle'\n",
    "    )\n",
    "        \n",
    "    level_0 = root.create_array(\n",
    "        '0',\n",
    "        shape=shape,\n",
    "        chunks=target_chunks,  # Large Z-slabs\n",
    "        dtype=dtype,\n",
    "        compressors=compressor,\n",
    "        chunk_key_encoding={\"name\": \"v2\", \"separator\": \"/\"},\n",
    "        overwrite=True\n",
    "    )\n",
    "        \n",
    "    # Copy in large slabs (efficient for contiguous HDF5)\n",
    "\n",
    "    level0_start = time.time()\n",
    "    \n",
    "    slab_size = target_chunks[0]\n",
    "    for z_start in range(0, shape[0], slab_size):\n",
    "        z_end = min(z_start + slab_size, shape[0])\n",
    "        print(f\"  Reading Z planes {z_start}-{z_end}\")\n",
    "            \n",
    "        slab = dataset[z_start:z_end, :, :]\n",
    "        level_0[z_start:z_end, :, :] = slab\n",
    "        del slab\n",
    "\n",
    "    elapsed_level0 = time.time() - level0_start\n",
    "\n",
    "    print(f\"\\nTiming breakdown:\")\n",
    "    print(f\"  Level 0 write: {elapsed_level0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "013c3ed9-2cb4-4346-8df1-6d6ddc5f8bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Source chunks: (64, 64, 64)\n",
      "  Source shape: (150, 3768, 2008)\n"
     ]
    }
   ],
   "source": [
    "# Inspection level_0\n",
    "root = zarr.open_group(output_path, mode='r')\n",
    "# Access level 0\n",
    "level_0 = root['0']\n",
    "\n",
    "# Wrap in Dask\n",
    "source = da.from_array(level_0, chunks=level_0.chunks)\n",
    "\n",
    "print(f\"  Source chunks: {source.chunksize}\")\n",
    "print(f\"  Source shape: {source.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8a3eb80-abf9-4eec-a60a-19aa76a25f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Building OME-Zarr Multi-Resolution Pyramid\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "LEVEL 1: Downsampling\n",
      "============================================================\n",
      "Previous shape: (150, 3768, 2008)\n",
      "New shape: (75, 1884, 1004)\n",
      "Reading with chunks: (128, 128, 128)\n",
      "Target chunks: (64, 64, 64)\n",
      "Estimated memory: 0.08 GB\n",
      "Writing level 1...\n",
      "✓ Level 1 complete: shape=(75, 1884, 1004), chunks=(64, 64, 64)\n",
      "\n",
      "============================================================\n",
      "LEVEL 2: Downsampling\n",
      "============================================================\n",
      "Previous shape: (75, 1884, 1004)\n",
      "New shape: (37, 942, 502)\n",
      "Reading with chunks: (75, 128, 128)\n",
      "Target chunks: (37, 64, 64)\n",
      "Estimated memory: 0.05 GB\n",
      "Writing level 2...\n",
      "✓ Level 2 complete: shape=(37, 942, 502), chunks=(37, 64, 64)\n",
      "\n",
      "============================================================\n",
      "LEVEL 3: Downsampling\n",
      "============================================================\n",
      "Previous shape: (37, 942, 502)\n",
      "New shape: (18, 471, 251)\n",
      "Reading with chunks: (37, 128, 128)\n",
      "Target chunks: (18, 64, 64)\n",
      "Estimated memory: 0.02 GB\n",
      "Writing level 3...\n",
      "✓ Level 3 complete: shape=(18, 471, 251), chunks=(18, 64, 64)\n",
      "\n",
      "============================================================\n",
      "Pyramid Complete!\n",
      "============================================================\n",
      "\n",
      "Timing breakdown:\n",
      "  Pyramid write: 6.4s\n",
      "  Full multiscale image write: 16.6s\n",
      "  Total runtime: 0:00:16\n"
     ]
    }
   ],
   "source": [
    "# Configure dask for memory constraints\n",
    "dask.config.set({\n",
    "    'array.chunk-size': f'{int(max_mem_gb * 0.3)}GB',\n",
    "    'distributed.worker.memory.target': 0.7,\n",
    "    'distributed.worker.memory.spill': 0.8,\n",
    "})\n",
    "    \n",
    "print(\"=\" * 60)\n",
    "print(\"Building OME-Zarr Multi-Resolution Pyramid\")\n",
    "print(\"=\" * 60)\n",
    "    \n",
    "# Compressor for all levels\n",
    "codecs = [\n",
    "    BytesCodec(),\n",
    "    BloscCodec(cname='zstd', clevel=compression_level, shuffle='bitshuffle')\n",
    "]\n",
    "\n",
    "    \n",
    "# Load input zarr\n",
    "root = zarr.open_group(output_path, mode='r')\n",
    "\n",
    "# Level 0\n",
    "level0 = root['0']\n",
    "\n",
    "# Wrap in Dask\n",
    "source = da.from_array(level0, chunks=level0.chunks)\n",
    "\n",
    "source_shape = source.shape\n",
    "source_chunks = source.chunksize\n",
    "    \n",
    "# ===== PYRAMID LEVELS: Build each level from previous =====\n",
    "current_shape = source_shape\n",
    "\n",
    "pyramid_start = time.time()\n",
    "    \n",
    "for level in range(1, pyramid_levels):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"LEVEL {level}: Downsampling\")\n",
    "    print(f\"{'='*60}\")\n",
    "        \n",
    "    # Calculate new shape after downsampling\n",
    "    new_shape = tuple(max(1, s // downsample_factor) for s in current_shape)\n",
    "        \n",
    "    print(f\"Previous shape: {current_shape}\")\n",
    "    print(f\"New shape: {new_shape}\")\n",
    "        \n",
    "    # Calculate optimal chunk size for reading previous level\n",
    "    # We want chunks that align well with both source and target\n",
    "    read_chunks = tuple(\n",
    "        min(c * downsample_factor, s) \n",
    "        for c, s in zip(target_chunks, current_shape)\n",
    "    )\n",
    "    \n",
    "    # Load previous level\n",
    "    # prev_array = da.from_zarr(store, component=str(level - 1))\n",
    "    prev_array = da.from_array(root[str(level-1)], chunks=read_chunks)\n",
    "        \n",
    "    # Rechunk previous level for efficient downsampling\n",
    "    prev_array = prev_array.rechunk(read_chunks)\n",
    "        \n",
    "    print(f\"Reading with chunks: {read_chunks}\")\n",
    "    \n",
    "    # Downsample using coarsen (block mean)\n",
    "    # This is memory efficient and HDD-friendly\n",
    "    downsampled = da.coarsen(\n",
    "        np.mean,\n",
    "        prev_array,\n",
    "        {0: downsample_factor, 1: downsample_factor, 2: downsample_factor},\n",
    "        trim_excess=True\n",
    "    ).astype(prev_array.dtype)\n",
    "        \n",
    "    # Rechunk to target chunks\n",
    "    # Adjust chunk size if array is smaller than target chunks\n",
    "    level_chunks = tuple(min(tc, ns) for tc, ns in zip(target_chunks, new_shape))\n",
    "    downsampled = downsampled.rechunk(level_chunks)\n",
    "        \n",
    "    print(f\"Target chunks: {level_chunks}\")\n",
    "        \n",
    "    # Calculate memory for this level\n",
    "    level_mem_gb = np.prod(read_chunks) * prev_array.dtype.itemsize * 10 / 1e9\n",
    "    print(f\"Estimated memory: {level_mem_gb:.2f} GB\")\n",
    "        \n",
    "    # Write to zarr\n",
    "    print(f\"Writing level {level}...\")\n",
    "    downsampled.to_zarr(\n",
    "        store,\n",
    "        component=str(level),\n",
    "        chunk_key_encoding={\"name\": \"v2\", \"separator\": \"/\"},\n",
    "        codecs=codecs,\n",
    "        overwrite=True\n",
    "    )\n",
    "        \n",
    "    print(f\"✓ Level {level} complete: shape={new_shape}, chunks={level_chunks}\")\n",
    "        \n",
    "    # Update current shape for next iteration\n",
    "    current_shape = new_shape\n",
    "\n",
    "elapsed_pyramid = time.time() - pyramid_start\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Pyramid Complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_seconds = elapsed_pyramid + elapsed_level0\n",
    "\n",
    "print(f\"\\nTiming breakdown:\")\n",
    "print(f\"  Pyramid write: {elapsed_pyramid:.1f}s\")\n",
    "print(f\"  Full multiscale image write: {total_seconds:.1f}s\")\n",
    "print(f\"  Total runtime: {timedelta(seconds=int(total_seconds))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4dfddd5-52c3-4785-b268-afc604956f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Adding OME-Zarr Metadata\n",
      "============================================================\n",
      "\n",
      "Pyramid Summary:\n",
      "Number of levels: 4\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'0'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m level \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pyramid_levels):\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     arr = \u001b[43mzarr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     56\u001b[39m     size_gb = np.prod(arr.shape) * arr.dtype.itemsize / \u001b[32m1e9\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Level \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, chunks=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr.chunks\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_gb\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m\"\u001b[39m)    \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/zarr/core/group.py:1782\u001b[39m, in \u001b[36mGroup.__getitem__\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   1755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) -> Array | Group:\n\u001b[32m   1756\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Obtain a group member.\u001b[39;00m\n\u001b[32m   1757\u001b[39m \n\u001b[32m   1758\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1780\u001b[39m \n\u001b[32m   1781\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1782\u001b[39m     obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_async_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, AsyncArray):\n\u001b[32m   1784\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Array(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/zarr/core/sync.py:187\u001b[39m, in \u001b[36mSyncMixin._sync\u001b[39m\u001b[34m(self, coroutine)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sync\u001b[39m(\u001b[38;5;28mself\u001b[39m, coroutine: Coroutine[Any, Any, T]) -> T:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# TODO: refactor this to to take *args and **kwargs and pass those to the method\u001b[39;00m\n\u001b[32m    186\u001b[39m     \u001b[38;5;66;03m# this should allow us to better type the sync wrapper\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoroutine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43masync.timeout\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/zarr/core/sync.py:142\u001b[39m, in \u001b[36msync\u001b[39m\u001b[34m(coro, loop, timeout)\u001b[39m\n\u001b[32m    139\u001b[39m return_result = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(finished)).result()\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/zarr/core/sync.py:98\u001b[39m, in \u001b[36m_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[33;03mAwait a coroutine and return the result of running it. If awaiting the coroutine raises an\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03mexception, the exception will be returned.\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/jlab/lib/python3.12/site-packages/zarr/core/group.py:695\u001b[39m, in \u001b[36mAsyncGroup.getitem\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    688\u001b[39m zgroup_bytes, zarray_bytes, zattrs_bytes = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    689\u001b[39m     (store_path / ZGROUP_JSON).get(),\n\u001b[32m    690\u001b[39m     (store_path / ZARRAY_JSON).get(),\n\u001b[32m    691\u001b[39m     (store_path / ZATTRS_JSON).get(),\n\u001b[32m    692\u001b[39m )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m zgroup_bytes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m zarray_bytes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    697\u001b[39m \u001b[38;5;66;03m# unpack the zarray, if this is None then we must be opening a group\u001b[39;00m\n\u001b[32m    698\u001b[39m zarray = json.loads(zarray_bytes.to_bytes()) \u001b[38;5;28;01mif\u001b[39;00m zarray_bytes \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: '0'"
     ]
    }
   ],
   "source": [
    "# ===== ADD OME-ZARR METADATA =====\n",
    "\n",
    "\n",
    "\n",
    "store = LocalStore(output_path)\n",
    "\n",
    "root = zarr.open_group(\n",
    "    store,\n",
    "    mode=\"a\",\n",
    "    zarr_version=2\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Adding OME-Zarr Metadata\")\n",
    "print(f\"{'='*60}\")\n",
    "    \n",
    "# Build datasets list\n",
    "datasets = []\n",
    "for level in range(pyramid_levels):\n",
    "    scale_factor = downsample_factor ** level\n",
    "    datasets.append({\n",
    "        'path': str(level),\n",
    "        'coordinateTransformations': [{\n",
    "            'type': 'scale',\n",
    "            'scale': [\n",
    "                float(scale_factor),  # z\n",
    "                float(scale_factor),  # y\n",
    "                float(scale_factor)   # x\n",
    "            ]\n",
    "        }]\n",
    "    })\n",
    "    \n",
    "# Add multiscales metadata\n",
    "root.attrs['multiscales'] = [{\n",
    "    'version': '0.4',\n",
    "    'name': 'pyramid',\n",
    "    'axes': [\n",
    "        {'name': 'z', 'type': 'space', 'unit': 'micrometer'},\n",
    "        {'name': 'y', 'type': 'space', 'unit': 'micrometer'},\n",
    "        {'name': 'x', 'type': 'space', 'unit': 'micrometer'}\n",
    "    ],\n",
    "    'datasets': datasets,\n",
    "    'type': 'mean',  # Downsampling method\n",
    "    'metadata': {\n",
    "        'description': 'Multi-resolution pyramid',\n",
    "        'method': 'block mean downsampling'\n",
    "    }\n",
    "}]\n",
    "\n",
    "print(\"\\nPyramid Summary:\")\n",
    "print(f\"Number of levels: {pyramid_levels}\")\n",
    "print(\"-\" * 60)\n",
    "    \n",
    "for level in range(pyramid_levels):\n",
    "    arr = zarr.open(store, mode='r')[str(level)]\n",
    "    size_gb = np.prod(arr.shape) * arr.dtype.itemsize / 1e9\n",
    "    print(f\"  Level {level}: shape={arr.shape}, chunks={arr.chunks}, size={size_gb:.2f} GB\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84b853ab-261f-45cc-ba5f-ad17f6d499f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OME-Zarr Structure Validation\n",
      "============================================================\n",
      "✓ Root is a group\n",
      "✓ Has multiscales metadata\n",
      "✓ Multiscales defines 4 datasets\n",
      "\n",
      "Dataset Check:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dataset 0: path='0'\n",
      "  ✓ Array exists\n",
      "    Shape: (150, 3768, 2008)\n",
      "    Chunks: (64, 64, 64)\n",
      "    Dtype: float32\n",
      "    Transforms: [{'type': 'scale', 'scale': [1.0, 1.0, 1.0]}]\n",
      "\n",
      "Dataset 1: path='1'\n",
      "  ✓ Array exists\n",
      "    Shape: (75, 1884, 1004)\n",
      "    Chunks: (64, 64, 64)\n",
      "    Dtype: float32\n",
      "    Transforms: [{'type': 'scale', 'scale': [2.0, 2.0, 2.0]}]\n",
      "\n",
      "Dataset 2: path='2'\n",
      "  ✓ Array exists\n",
      "    Shape: (37, 942, 502)\n",
      "    Chunks: (37, 64, 64)\n",
      "    Dtype: float32\n",
      "    Transforms: [{'type': 'scale', 'scale': [4.0, 4.0, 4.0]}]\n",
      "\n",
      "Dataset 3: path='3'\n",
      "  ✓ Array exists\n",
      "    Shape: (18, 471, 251)\n",
      "    Chunks: (18, 64, 64)\n",
      "    Dtype: float32\n",
      "    Transforms: [{'type': 'scale', 'scale': [8.0, 8.0, 8.0]}]\n",
      "\n",
      "============================================================\n",
      "Checking for unexpected arrays...\n",
      "============================================================\n",
      "✓ No unexpected arrays\n",
      "\n",
      "============================================================\n",
      "Full Multiscales Metadata:\n",
      "============================================================\n",
      "[\n",
      "  {\n",
      "    \"version\": \"0.4\",\n",
      "    \"name\": \"pyramid\",\n",
      "    \"axes\": [\n",
      "      {\n",
      "        \"name\": \"z\",\n",
      "        \"type\": \"space\",\n",
      "        \"unit\": \"micrometer\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"y\",\n",
      "        \"type\": \"space\",\n",
      "        \"unit\": \"micrometer\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"x\",\n",
      "        \"type\": \"space\",\n",
      "        \"unit\": \"micrometer\"\n",
      "      }\n",
      "    ],\n",
      "    \"datasets\": [\n",
      "      {\n",
      "        \"path\": \"0\",\n",
      "        \"coordinateTransformations\": [\n",
      "          {\n",
      "            \"type\": \"scale\",\n",
      "            \"scale\": [\n",
      "              1.0,\n",
      "              1.0,\n",
      "              1.0\n",
      "            ]\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"path\": \"1\",\n",
      "        \"coordinateTransformations\": [\n",
      "          {\n",
      "            \"type\": \"scale\",\n",
      "            \"scale\": [\n",
      "              2.0,\n",
      "              2.0,\n",
      "              2.0\n",
      "            ]\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"path\": \"2\",\n",
      "        \"coordinateTransformations\": [\n",
      "          {\n",
      "            \"type\": \"scale\",\n",
      "            \"scale\": [\n",
      "              4.0,\n",
      "              4.0,\n",
      "              4.0\n",
      "            ]\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"path\": \"3\",\n",
      "        \"coordinateTransformations\": [\n",
      "          {\n",
      "            \"type\": \"scale\",\n",
      "            \"scale\": [\n",
      "              8.0,\n",
      "              8.0,\n",
      "              8.0\n",
      "            ]\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"type\": \"mean\",\n",
      "    \"metadata\": {\n",
      "      \"description\": \"Multi-resolution pyramid\",\n",
      "      \"method\": \"block mean downsampling\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "import json\n",
    "\n",
    "def validate_ome_zarr_structure(path):\n",
    "    \"\"\"Check OME-Zarr v3 structure and print details nicely\"\"\"\n",
    "    root = zarr.open(path, mode='r')\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"OME-Zarr Structure Validation\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Root check\n",
    "    if hasattr(root, 'attrs') and hasattr(root, '__getitem__'):\n",
    "        print(\"✓ Root is a group\")\n",
    "    else:\n",
    "        print(\"❌ Root is not a group!\")\n",
    "        return False\n",
    "\n",
    "    # Multiscales metadata\n",
    "    if 'multiscales' not in root.attrs:\n",
    "        print(\"❌ Missing 'multiscales' attribute!\")\n",
    "        return False\n",
    "    print(\"✓ Has multiscales metadata\")\n",
    "\n",
    "    multiscales = root.attrs['multiscales']\n",
    "    if not isinstance(multiscales, list) or len(multiscales) == 0:\n",
    "        print(\"❌ Multiscales is not a list or empty!\")\n",
    "        return False\n",
    "\n",
    "    ms = multiscales[0]\n",
    "    datasets = ms.get('datasets', [])\n",
    "    print(f\"✓ Multiscales defines {len(datasets)} datasets\")\n",
    "\n",
    "    # Dataset check\n",
    "    print(\"\\nDataset Check:\")\n",
    "    print(\"-\"*60)\n",
    "    for i, ds in enumerate(datasets):\n",
    "        path_name = ds.get('path')\n",
    "        print(f\"\\nDataset {i}: path='{path_name}'\")\n",
    "\n",
    "        if path_name not in root:\n",
    "            print(f\"  ❌ Array '{path_name}' not found!\")\n",
    "            return False\n",
    "\n",
    "        arr = root[path_name]\n",
    "\n",
    "        # Array validation (duck typing)\n",
    "        if not hasattr(arr, 'shape') or not hasattr(arr, 'chunks') or not hasattr(arr, 'dtype'):\n",
    "            print(f\"  ❌ '{path_name}' is not array-like!\")\n",
    "            return False\n",
    "\n",
    "        print(f\"  ✓ Array exists\")\n",
    "        print(f\"    Shape: {arr.shape}\")\n",
    "        print(f\"    Chunks: {arr.chunks}\")\n",
    "        print(f\"    Dtype: {arr.dtype}\")\n",
    "\n",
    "        # Coordinate transformations\n",
    "        if 'coordinateTransformations' in ds:\n",
    "            transforms = ds['coordinateTransformations']\n",
    "            print(f\"    Transforms: {transforms}\")\n",
    "\n",
    "    # Extra/missing arrays\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Checking for unexpected arrays...\")\n",
    "    print(\"=\"*60)\n",
    "    expected_paths = {ds['path'] for ds in datasets}\n",
    "    actual_paths = set(root.array_keys())\n",
    "    extra = actual_paths - expected_paths\n",
    "    missing = expected_paths - actual_paths\n",
    "\n",
    "    if extra:\n",
    "        print(f\"⚠️  Found unexpected arrays: {extra}\")\n",
    "    else:\n",
    "        print(\"✓ No unexpected arrays\")\n",
    "\n",
    "    if missing:\n",
    "        print(f\"❌ Missing expected arrays: {missing}\")\n",
    "        return False\n",
    "\n",
    "    # Full metadata\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Full Multiscales Metadata:\")\n",
    "    print(\"=\"*60)\n",
    "    print(json.dumps(multiscales, indent=2))\n",
    "\n",
    "# Run validator\n",
    "validate_ome_zarr_structure(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facce711-89ce-4938-a8e1-e8bd051f989d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
